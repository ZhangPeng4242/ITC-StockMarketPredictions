{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock_Prediction_Transformers_Multistocks_Classification_Changes",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65cf9df7a7eb413a8759f57c82801a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b60ae3aed3f48c3abaffeb6fc162b3e",
              "IPY_MODEL_d555fa52a61e4e34bd438788f41a2655",
              "IPY_MODEL_a06678a64cf6476b82db113bb96acdc7"
            ],
            "layout": "IPY_MODEL_a2614dc3d34643609f93e6d20822bbcb"
          }
        },
        "1b60ae3aed3f48c3abaffeb6fc162b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ff45c47f20441338a1b5dee8eac9dd3",
            "placeholder": "​",
            "style": "IPY_MODEL_92d16ae3ac4e4c80bce7c9d523c65071",
            "value": "100%"
          }
        },
        "d555fa52a61e4e34bd438788f41a2655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d56b179ba8b4edabef02ea980dc3f1c",
            "max": 55,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b9bd132637641acb29dc4eb0006c296",
            "value": 55
          }
        },
        "a06678a64cf6476b82db113bb96acdc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebde66f4704b47339f18f85fc888318d",
            "placeholder": "​",
            "style": "IPY_MODEL_7aa53fa8c55049689e960e1db764f476",
            "value": " 55/55 [00:33&lt;00:00,  1.01it/s]"
          }
        },
        "a2614dc3d34643609f93e6d20822bbcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ff45c47f20441338a1b5dee8eac9dd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92d16ae3ac4e4c80bce7c9d523c65071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d56b179ba8b4edabef02ea980dc3f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9bd132637641acb29dc4eb0006c296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebde66f4704b47339f18f85fc888318d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aa53fa8c55049689e960e1db764f476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This is to control wether we load trained model or train the model from scratch\n",
        "LOAD_MODEL = False"
      ],
      "metadata": {
        "id": "ZMoDRLcDe9aR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDc4wZema-Ca",
        "outputId": "721bcd93-5171-43e7-ab04-14d8fef03f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.72-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Collecting requests>=2.26\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 16.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.9.1 requests-2.28.1 yfinance-0.1.72\n",
            "Tensorflow version: 2.8.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, datetime\n",
        "!pip install yfinance\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "print('Tensorflow version: {}'.format(tf.__version__))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(df):\n",
        "  # '''Calculate percentage change'''\n",
        "  # df['Open'] = df['Open'].pct_change() # Create arithmetic returns column\n",
        "  # df['High'] = df['High'].pct_change() # Create arithmetic returns column\n",
        "  # df['Low'] = df['Low'].pct_change() # Create arithmetic returns column\n",
        "  # df['Close'] = df['Close'].pct_change() # Create arithmetic returns column\n",
        "  # df['Volume'] = df['Volume'].pct_change()\n",
        "  #  df['Close'] = df['Close'].diff()\n",
        "  # df.dropna(how='any', axis=0, inplace=True) # Drop all rows with NaN values\n",
        "  # df.loc[df['Close']>=0,'Close']=1\n",
        "  # df.loc[df['Close']<0,'Close']=0\n",
        "  df = df.diff()\n",
        "  df['diff']=df['Close']\n",
        "  df.dropna(how='any', axis=0, inplace=True) # Drop all rows with NaN values\n",
        "\n",
        "  ###############################################################################\n",
        "  '''Create indexes to split dataset'''\n",
        "\n",
        "  times = sorted(df.index.values)\n",
        "  last_10pct = sorted(df.index.values)[-int(0.1*len(times))] # Last 10% of series\n",
        "  last_20pct = sorted(df.index.values)[-int(0.2*len(times))] # Last 20% of series\n",
        "\n",
        "  ###############################################################################\n",
        "  '''Normalize price columns'''\n",
        "  \n",
        "  df = df.dropna()\n",
        "  df = df[df!=np.inf]\n",
        "  min_return = min(df[(df.index < last_20pct)][['Open', 'High', 'Low', 'Close']].min(axis=0))\n",
        "  max_return = max(df[(df.index < last_20pct)][['Open', 'High', 'Low', 'Close']].max(axis=0))\n",
        "\n",
        "  # Min-max normalize price columns (0-1 range)\n",
        "  df['Open'] = (df['Open'] - min_return) / (max_return - min_return)\n",
        "  df['High'] = (df['High'] - min_return) / (max_return - min_return)\n",
        "  df['Low'] = (df['Low'] - min_return) / (max_return - min_return)\n",
        "  df['Close'] = (df['Close'] - min_return) / (max_return - min_return)\n",
        "\n",
        "  ###############################################################################\n",
        "  '''Normalize volume column'''\n",
        "  df = df.dropna()\n",
        "  df = df[df!=np.inf]\n",
        "  min_volume = df[(df.index < last_20pct)]['Volume'].min(axis=0)\n",
        "  max_volume = df[(df.index < last_20pct)]['Volume'].max(axis=0)\n",
        "\n",
        "  # Min-max normalize volume columns (0-1 range)\n",
        "  df['Volume'] = (df['Volume'] - min_volume) / (max_volume - min_volume)\n",
        "\n",
        "  ###############################################################################\n",
        "  '''Create training, validation and test split'''\n",
        "\n",
        "  df = df.dropna()\n",
        "  df = df[df!=np.inf]\n",
        "  df_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\n",
        "  df_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\n",
        "  df_test = df[(df.index >= last_10pct)]\n",
        "\n",
        "  # Remove date column\n",
        "  df_train.drop(columns=['Date'], inplace=True)\n",
        "  df_val.drop(columns=['Date'], inplace=True)\n",
        "  df_test.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "  # Convert pandas columns into arrays\n",
        "  train_data = df_train.values\n",
        "  val_data = df_val.values\n",
        "  test_data = df_test.values\n",
        "\n",
        "  print('Training data shape: {}'.format(train_data.shape))\n",
        "  print('Validation data shape: {}'.format(val_data.shape))\n",
        "  print('Test data shape: {}'.format(test_data.shape))\n",
        "  return (train_data,val_data,test_data,(min_return,max_return))"
      ],
      "metadata": {
        "id": "nMFPFDmtbcVl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(train_data,val_data,test_data):\n",
        "  # Training data\n",
        "  X_train, y_train = [], []\n",
        "  for i in range(seq_len, len(train_data)):\n",
        "    X_train.append(train_data[i-seq_len:i]) # Chunks of training data with a length of 128 df-rows\n",
        "    y_train.append(train_data[:, 5][i]) #Value of 4th column (Close Price) of df-row 128+1\n",
        "  X_train, y_train = np.array(X_train)[:,:,:5], np.array(y_train)\n",
        "  y_train[y_train>=0]=1\n",
        "  y_train[y_train<0]=0\n",
        "  ###############################################################################\n",
        "\n",
        "  # Validation data\n",
        "  X_val, y_val = [], []\n",
        "  for i in range(seq_len, len(val_data)):\n",
        "      X_val.append(val_data[i-seq_len:i])\n",
        "      y_val.append(val_data[:, 5][i])\n",
        "  X_val, y_val = np.array(X_val)[:,:,:5], np.array(y_val)\n",
        "  y_val[y_val>=0]=1\n",
        "  y_val[y_val<0]=0\n",
        "  ###############################################################################\n",
        "\n",
        "  # Test data\n",
        "  X_test, y_test = [], []\n",
        "  for i in range(seq_len, len(test_data)):\n",
        "      X_test.append(test_data[i-seq_len:i])\n",
        "      y_test.append(test_data[:, 5][i])    \n",
        "  X_test, y_test = np.array(X_test)[:,:,:5], np.array(y_test)\n",
        "  y_test[y_test>=0]=1\n",
        "  y_test[y_test<0]=0\n",
        "  print('Training set shape', X_train.shape, y_train.shape)\n",
        "  print('Validation set shape', X_val.shape, y_val.shape)\n",
        "  print('Testing set shape' ,X_test.shape, y_test.shape)\n",
        "  return (X_train,y_train,X_val,y_val,X_test,y_test)"
      ],
      "metadata": {
        "id": "WVcUU45-cP9S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "seq_len = 128\n",
        "\n",
        "d_k = 256\n",
        "d_v = 256\n",
        "n_heads = 12\n",
        "ff_dim = 256"
      ],
      "metadata": {
        "id": "MkALJPyCbIlG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tickers.txt') as tfile:\n",
        "  tickers = tfile.read()\n",
        "tickers = tickers.split()"
      ],
      "metadata": {
        "id": "zpq2QFpObJxB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combining different stocks to train, val, test set\n",
        "np.random.seed(42)\n",
        "data_history = []\n",
        "first=True\n",
        "for ticker in tqdm(np.random.choice(tickers,size=50, replace=False).tolist()+['AAPL','IBM','QQQ','NVDA','TSLA'],total=55):\n",
        "  print(ticker)\n",
        "  df = yf.download(ticker).reset_index()\n",
        "  df = df.drop(columns='Close')\n",
        "  df.columns=['Date','Open','High','Low','Close','Volume']\n",
        "  if len(df.index)==0:\n",
        "    print(\"Cannot featch the data! Ticker not found.\")\n",
        "    continue\n",
        "  if len(df.index)<=seq_len*10:\n",
        "    print(\"Stock Data Not Big Enough!\")\n",
        "    continue\n",
        "    \n",
        "  train_data,val_data,test_data,min_max = preprocessing(df)\n",
        "  data_history.append((ticker,min_max,(len(train_data),len(val_data),len(test_data))))\n",
        "  X_train,y_train,X_val,y_val,X_test,y_test = train_test_split(train_data,val_data,test_data)\n",
        "  if first:\n",
        "    X_train_all,y_train_all,X_val_all,y_val_all,X_test_all,y_test_all = X_train,y_train,X_val,y_val,X_test,y_test\n",
        "    first=False\n",
        "    continue\n",
        "  X_train_all,y_train_all,X_val_all,y_val_all,X_test_all,y_test_all\\\n",
        "        = np.vstack((X_train_all,X_train)), np.concatenate((y_train_all,y_train)),\\\n",
        "          np.vstack((X_val_all,X_val)), np.concatenate((y_val_all,y_val)),\\\n",
        "          np.vstack((X_test_all,X_test)), np.concatenate((y_test_all,y_test)),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "65cf9df7a7eb413a8759f57c82801a1f",
            "1b60ae3aed3f48c3abaffeb6fc162b3e",
            "d555fa52a61e4e34bd438788f41a2655",
            "a06678a64cf6476b82db113bb96acdc7",
            "a2614dc3d34643609f93e6d20822bbcb",
            "2ff45c47f20441338a1b5dee8eac9dd3",
            "92d16ae3ac4e4c80bce7c9d523c65071",
            "3d56b179ba8b4edabef02ea980dc3f1c",
            "3b9bd132637641acb29dc4eb0006c296",
            "ebde66f4704b47339f18f85fc888318d",
            "7aa53fa8c55049689e960e1db764f476"
          ]
        },
        "id": "87BJEu-8cRIV",
        "outputId": "ad091dc1-89be-4113-c852-dc173c43a284"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/55 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65cf9df7a7eb413a8759f57c82801a1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BE\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "ZDGE\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1225, 6)\n",
            "Validation data shape: (153, 6)\n",
            "Test data shape: (153, 6)\n",
            "Training set shape (1097, 128, 5) (1097,)\n",
            "Validation set shape (25, 128, 5) (25,)\n",
            "Testing set shape (25, 128, 5) (25,)\n",
            "KFY\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (4709, 6)\n",
            "Validation data shape: (589, 6)\n",
            "Test data shape: (588, 6)\n",
            "Training set shape (4581, 128, 5) (4581,)\n",
            "Validation set shape (461, 128, 5) (461,)\n",
            "Testing set shape (460, 128, 5) (460,)\n",
            "SUM\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1473, 6)\n",
            "Validation data shape: (184, 6)\n",
            "Test data shape: (184, 6)\n",
            "Training set shape (1345, 128, 5) (1345,)\n",
            "Validation set shape (56, 128, 5) (56,)\n",
            "Testing set shape (56, 128, 5) (56,)\n",
            "CIM\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2939, 6)\n",
            "Validation data shape: (367, 6)\n",
            "Test data shape: (367, 6)\n",
            "Training set shape (2811, 128, 5) (2811,)\n",
            "Validation set shape (239, 128, 5) (239,)\n",
            "Testing set shape (239, 128, 5) (239,)\n",
            "EME\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5536, 6)\n",
            "Validation data shape: (692, 6)\n",
            "Test data shape: (691, 6)\n",
            "Training set shape (5408, 128, 5) (5408,)\n",
            "Validation set shape (564, 128, 5) (564,)\n",
            "Testing set shape (563, 128, 5) (563,)\n",
            "MUI\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (3812, 6)\n",
            "Validation data shape: (477, 6)\n",
            "Test data shape: (476, 6)\n",
            "Training set shape (3684, 128, 5) (3684,)\n",
            "Validation set shape (349, 128, 5) (349,)\n",
            "Testing set shape (348, 128, 5) (348,)\n",
            "RGT\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1759, 6)\n",
            "Validation data shape: (220, 6)\n",
            "Test data shape: (219, 6)\n",
            "Training set shape (1631, 128, 5) (1631,)\n",
            "Validation set shape (92, 128, 5) (92,)\n",
            "Testing set shape (91, 128, 5) (91,)\n",
            "BGX\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2302, 6)\n",
            "Validation data shape: (288, 6)\n",
            "Test data shape: (287, 6)\n",
            "Training set shape (2174, 128, 5) (2174,)\n",
            "Validation set shape (160, 128, 5) (160,)\n",
            "Testing set shape (159, 128, 5) (159,)\n",
            "GOED\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "RES\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (7676, 6)\n",
            "Validation data shape: (959, 6)\n",
            "Test data shape: (959, 6)\n",
            "Training set shape (7548, 128, 5) (7548,)\n",
            "Validation set shape (831, 128, 5) (831,)\n",
            "Testing set shape (831, 128, 5) (831,)\n",
            "CYD\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5548, 6)\n",
            "Validation data shape: (693, 6)\n",
            "Test data shape: (693, 6)\n",
            "Training set shape (5420, 128, 5) (5420,)\n",
            "Validation set shape (565, 128, 5) (565,)\n",
            "Testing set shape (565, 128, 5) (565,)\n",
            "SNMP\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (3147, 6)\n",
            "Validation data shape: (393, 6)\n",
            "Test data shape: (393, 6)\n",
            "Training set shape (3019, 128, 5) (3019,)\n",
            "Validation set shape (265, 128, 5) (265,)\n",
            "Testing set shape (265, 128, 5) (265,)\n",
            "EIX\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (9921, 6)\n",
            "Validation data shape: (1240, 6)\n",
            "Test data shape: (1240, 6)\n",
            "Training set shape (9793, 128, 5) (9793,)\n",
            "Validation set shape (1112, 128, 5) (1112,)\n",
            "Testing set shape (1112, 128, 5) (1112,)\n",
            "SILV\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "BCSF\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "TPZ\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2604, 6)\n",
            "Validation data shape: (326, 6)\n",
            "Test data shape: (325, 6)\n",
            "Training set shape (2476, 128, 5) (2476,)\n",
            "Validation set shape (198, 128, 5) (198,)\n",
            "Testing set shape (197, 128, 5) (197,)\n",
            "TDC\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2972, 6)\n",
            "Validation data shape: (372, 6)\n",
            "Test data shape: (371, 6)\n",
            "Training set shape (2844, 128, 5) (2844,)\n",
            "Validation set shape (244, 128, 5) (244,)\n",
            "Testing set shape (243, 128, 5) (243,)\n",
            "NBB\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2454, 6)\n",
            "Validation data shape: (307, 6)\n",
            "Test data shape: (306, 6)\n",
            "Training set shape (2326, 128, 5) (2326,)\n",
            "Validation set shape (179, 128, 5) (179,)\n",
            "Testing set shape (178, 128, 5) (178,)\n",
            "JMP\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "\n",
            "1 Failed download:\n",
            "- JMP: No data found, symbol may be delisted\n",
            "Cannot featch the data! Ticker not found.\n",
            "NGL\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2242, 6)\n",
            "Validation data shape: (280, 6)\n",
            "Test data shape: (280, 6)\n",
            "Training set shape (2114, 128, 5) (2114,)\n",
            "Validation set shape (152, 128, 5) (152,)\n",
            "Testing set shape (152, 128, 5) (152,)\n",
            "FHN\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8532, 6)\n",
            "Validation data shape: (1067, 6)\n",
            "Test data shape: (1066, 6)\n",
            "Training set shape (8404, 128, 5) (8404,)\n",
            "Validation set shape (939, 128, 5) (939,)\n",
            "Testing set shape (938, 128, 5) (938,)\n",
            "FEI\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1932, 6)\n",
            "Validation data shape: (242, 6)\n",
            "Test data shape: (241, 6)\n",
            "Training set shape (1804, 128, 5) (1804,)\n",
            "Validation set shape (114, 128, 5) (114,)\n",
            "Testing set shape (113, 128, 5) (113,)\n",
            "ZTO\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1144, 6)\n",
            "Validation data shape: (143, 6)\n",
            "Test data shape: (142, 6)\n",
            "Training set shape (1016, 128, 5) (1016,)\n",
            "Validation set shape (15, 128, 5) (15,)\n",
            "Testing set shape (14, 128, 5) (14,)\n",
            "MSD\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5836, 6)\n",
            "Validation data shape: (729, 6)\n",
            "Test data shape: (729, 6)\n",
            "Training set shape (5708, 128, 5) (5708,)\n",
            "Validation set shape (601, 128, 5) (601,)\n",
            "Testing set shape (601, 128, 5) (601,)\n",
            "JHS\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8532, 6)\n",
            "Validation data shape: (1067, 6)\n",
            "Test data shape: (1066, 6)\n",
            "Training set shape (8404, 128, 5) (8404,)\n",
            "Validation set shape (939, 128, 5) (939,)\n",
            "Testing set shape (938, 128, 5) (938,)\n",
            "XIN\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2932, 6)\n",
            "Validation data shape: (366, 6)\n",
            "Test data shape: (366, 6)\n",
            "Training set shape (2804, 128, 5) (2804,)\n",
            "Validation set shape (238, 128, 5) (238,)\n",
            "Testing set shape (238, 128, 5) (238,)\n",
            "WMT\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (10057, 6)\n",
            "Validation data shape: (1257, 6)\n",
            "Test data shape: (1257, 6)\n",
            "Training set shape (9929, 128, 5) (9929,)\n",
            "Validation set shape (1129, 128, 5) (1129,)\n",
            "Testing set shape (1129, 128, 5) (1129,)\n",
            "YPF\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5845, 6)\n",
            "Validation data shape: (731, 6)\n",
            "Test data shape: (730, 6)\n",
            "Training set shape (5717, 128, 5) (5717,)\n",
            "Validation set shape (603, 128, 5) (603,)\n",
            "Testing set shape (602, 128, 5) (602,)\n",
            "RVLV\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "PCPL\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "\n",
            "1 Failed download:\n",
            "- PCPL: No data found, symbol may be delisted\n",
            "Cannot featch the data! Ticker not found.\n",
            "GTN\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (3996, 6)\n",
            "Validation data shape: (499, 6)\n",
            "Test data shape: (499, 6)\n",
            "Training set shape (3868, 128, 5) (3868,)\n",
            "Validation set shape (371, 128, 5) (371,)\n",
            "Testing set shape (371, 128, 5) (371,)\n",
            "POST\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2100, 6)\n",
            "Validation data shape: (263, 6)\n",
            "Test data shape: (262, 6)\n",
            "Training set shape (1972, 128, 5) (1972,)\n",
            "Validation set shape (135, 128, 5) (135,)\n",
            "Testing set shape (134, 128, 5) (134,)\n",
            "LH\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (6503, 6)\n",
            "Validation data shape: (813, 6)\n",
            "Test data shape: (812, 6)\n",
            "Training set shape (6375, 128, 5) (6375,)\n",
            "Validation set shape (685, 128, 5) (685,)\n",
            "Testing set shape (684, 128, 5) (684,)\n",
            "SXI\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (9961, 6)\n",
            "Validation data shape: (1245, 6)\n",
            "Test data shape: (1245, 6)\n",
            "Training set shape (9833, 128, 5) (9833,)\n",
            "Validation set shape (1117, 128, 5) (1117,)\n",
            "Testing set shape (1117, 128, 5) (1117,)\n",
            "AZEK\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "EPM\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5282, 6)\n",
            "Validation data shape: (660, 6)\n",
            "Test data shape: (660, 6)\n",
            "Training set shape (5154, 128, 5) (5154,)\n",
            "Validation set shape (532, 128, 5) (532,)\n",
            "Testing set shape (532, 128, 5) (532,)\n",
            "GOF\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (3008, 6)\n",
            "Validation data shape: (376, 6)\n",
            "Test data shape: (376, 6)\n",
            "Training set shape (2880, 128, 5) (2880,)\n",
            "Validation set shape (248, 128, 5) (248,)\n",
            "Testing set shape (248, 128, 5) (248,)\n",
            "HDB\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (4217, 6)\n",
            "Validation data shape: (527, 6)\n",
            "Test data shape: (527, 6)\n",
            "Training set shape (4089, 128, 5) (4089,)\n",
            "Validation set shape (399, 128, 5) (399,)\n",
            "Testing set shape (399, 128, 5) (399,)\n",
            "PCM\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5812, 6)\n",
            "Validation data shape: (726, 6)\n",
            "Test data shape: (726, 6)\n",
            "Training set shape (5684, 128, 5) (5684,)\n",
            "Validation set shape (598, 128, 5) (598,)\n",
            "Testing set shape (598, 128, 5) (598,)\n",
            "BXMT\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8374, 6)\n",
            "Validation data shape: (1047, 6)\n",
            "Test data shape: (1046, 6)\n",
            "Training set shape (8246, 128, 5) (8246,)\n",
            "Validation set shape (919, 128, 5) (919,)\n",
            "Testing set shape (918, 128, 5) (918,)\n",
            "ACU\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8532, 6)\n",
            "Validation data shape: (1067, 6)\n",
            "Test data shape: (1066, 6)\n",
            "Training set shape (8404, 128, 5) (8404,)\n",
            "Validation set shape (939, 128, 5) (939,)\n",
            "Testing set shape (938, 128, 5) (938,)\n",
            "SHLX\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1546, 6)\n",
            "Validation data shape: (193, 6)\n",
            "Test data shape: (193, 6)\n",
            "Training set shape (1418, 128, 5) (1418,)\n",
            "Validation set shape (65, 128, 5) (65,)\n",
            "Testing set shape (65, 128, 5) (65,)\n",
            "DNB\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "MTR\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8617, 6)\n",
            "Validation data shape: (1077, 6)\n",
            "Test data shape: (1077, 6)\n",
            "Training set shape (8489, 128, 5) (8489,)\n",
            "Validation set shape (949, 128, 5) (949,)\n",
            "Testing set shape (949, 128, 5) (949,)\n",
            "DTQ\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "\n",
            "1 Failed download:\n",
            "- DTQ: No data found, symbol may be delisted\n",
            "Cannot featch the data! Ticker not found.\n",
            "ARGD\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1967, 6)\n",
            "Validation data shape: (246, 6)\n",
            "Test data shape: (245, 6)\n",
            "Training set shape (1839, 128, 5) (1839,)\n",
            "Validation set shape (118, 128, 5) (118,)\n",
            "Testing set shape (117, 128, 5) (117,)\n",
            "ASH\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8532, 6)\n",
            "Validation data shape: (1067, 6)\n",
            "Test data shape: (1066, 6)\n",
            "Training set shape (8404, 128, 5) (8404,)\n",
            "Validation set shape (939, 128, 5) (939,)\n",
            "Testing set shape (938, 128, 5) (938,)\n",
            "BGIO\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "\n",
            "1 Failed download:\n",
            "- BGIO: No data found, symbol may be delisted\n",
            "Cannot featch the data! Ticker not found.\n",
            "CVE\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2542, 6)\n",
            "Validation data shape: (318, 6)\n",
            "Test data shape: (317, 6)\n",
            "Training set shape (2414, 128, 5) (2414,)\n",
            "Validation set shape (190, 128, 5) (190,)\n",
            "Testing set shape (189, 128, 5) (189,)\n",
            "AAPL\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8382, 6)\n",
            "Validation data shape: (1048, 6)\n",
            "Test data shape: (1047, 6)\n",
            "Training set shape (8254, 128, 5) (8254,)\n",
            "Validation set shape (920, 128, 5) (920,)\n",
            "Testing set shape (919, 128, 5) (919,)\n",
            "IBM\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (12184, 6)\n",
            "Validation data shape: (1523, 6)\n",
            "Test data shape: (1523, 6)\n",
            "Training set shape (12056, 128, 5) (12056,)\n",
            "Validation set shape (1395, 128, 5) (1395,)\n",
            "Testing set shape (1395, 128, 5) (1395,)\n",
            "QQQ\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (4695, 6)\n",
            "Validation data shape: (587, 6)\n",
            "Test data shape: (586, 6)\n",
            "Training set shape (4567, 128, 5) (4567,)\n",
            "Validation set shape (459, 128, 5) (459,)\n",
            "Testing set shape (458, 128, 5) (458,)\n",
            "NVDA\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (4720, 6)\n",
            "Validation data shape: (590, 6)\n",
            "Test data shape: (589, 6)\n",
            "Training set shape (4592, 128, 5) (4592,)\n",
            "Validation set shape (462, 128, 5) (462,)\n",
            "Testing set shape (461, 128, 5) (461,)\n",
            "TSLA\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2420, 6)\n",
            "Validation data shape: (302, 6)\n",
            "Test data shape: (302, 6)\n",
            "Training set shape (2292, 128, 5) (2292,)\n",
            "Validation set shape (174, 128, 5) (174,)\n",
            "Testing set shape (174, 128, 5) (174,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all.shape,y_train_all.shape,X_val_all.shape,y_val_all.shape,X_test_all.shape,y_test_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm3OQO8ecj3x",
        "outputId": "b3672e4c-354f-443e-e4f6-8acccca571ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((212887, 128, 5),\n",
              " (212887,),\n",
              " (21684, 128, 5),\n",
              " (21684,),\n",
              " (21661, 128, 5),\n",
              " (21661,))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snu7LWaLrIP0",
        "outputId": "7fc92d58-ad68-416c-d5a9-8bf97c3640e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.29505578, 0.22807013, 0.31419456, 0.33333326, 0.49238524],\n",
              "        [0.31897924, 0.31961723, 0.3317384 , 0.32216907, 0.49383153],\n",
              "        [0.31897924, 0.34226471, 0.37480063, 0.36842102, 0.49416512],\n",
              "        ...,\n",
              "        [0.37480063, 0.3582137 , 0.35406696, 0.35725675, 0.49360533],\n",
              "        [0.33971287, 0.36044654, 0.35406696, 0.37320574, 0.49381211],\n",
              "        [0.36682612, 0.35247207, 0.35964911, 0.35247207, 0.49346139]],\n",
              "\n",
              "       [[0.31897924, 0.31961723, 0.3317384 , 0.32216907, 0.49383153],\n",
              "        [0.31897924, 0.34226471, 0.37480063, 0.36842102, 0.49416512],\n",
              "        [0.38755975, 0.77192979, 0.37161081, 0.61562992, 0.50030331],\n",
              "        ...,\n",
              "        [0.33971287, 0.36044654, 0.35406696, 0.37320574, 0.49381211],\n",
              "        [0.36682612, 0.35247207, 0.35964911, 0.35247207, 0.49346139],\n",
              "        [0.34768739, 0.35087714, 0.35964908, 0.34449756, 0.49358134]],\n",
              "\n",
              "       [[0.31897924, 0.34226471, 0.37480063, 0.36842102, 0.49416512],\n",
              "        [0.38755975, 0.77192979, 0.37161081, 0.61562992, 0.50030331],\n",
              "        [0.6236044 , 0.20733646, 0.40191384, 0.14673046, 0.48776874],\n",
              "        ...,\n",
              "        [0.36682612, 0.35247207, 0.35964911, 0.35247207, 0.49346139],\n",
              "        [0.34768739, 0.35087714, 0.35964908, 0.34449756, 0.49358134],\n",
              "        [0.35725675, 0.37304622, 0.35406696, 0.37320574, 0.49360191]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.41647544, 0.40776486, 0.4117745 , 0.40466784, 0.43773609],\n",
              "        [0.42761948, 0.42443944, 0.42123176, 0.42803425, 0.39423961],\n",
              "        [0.40643758, 0.42427351, 0.42145291, 0.42869786, 0.40672784],\n",
              "        ...,\n",
              "        [0.51447629, 0.78948096, 0.5346902 , 0.779222  , 1.        ],\n",
              "        [1.        , 0.92694215, 0.86475149, 0.7173632 , 0.67742001],\n",
              "        [0.25622875, 0.08116025, 0.06246725, 0.        , 0.19020918]],\n",
              "\n",
              "       [[0.42761948, 0.42443944, 0.42123176, 0.42803425, 0.39423961],\n",
              "        [0.40643758, 0.42427351, 0.42145291, 0.42869786, 0.40672784],\n",
              "        [0.44329847, 0.4385422 , 0.44025667, 0.43480915, 0.43046175],\n",
              "        ...,\n",
              "        [1.        , 0.92694215, 0.86475149, 0.7173632 , 0.67742001],\n",
              "        [0.25622875, 0.08116025, 0.06246725, 0.        , 0.19020918],\n",
              "        [0.08024779, 0.28263701, 0.37400094, 0.46074731, 0.26354338]],\n",
              "\n",
              "       [[0.40643758, 0.42427351, 0.42145291, 0.42869786, 0.40672784],\n",
              "        [0.44329847, 0.4385422 , 0.44025667, 0.43480915, 0.43046175],\n",
              "        [0.42573908, 0.41899186, 0.4245224 , 0.41221696, 0.39586434],\n",
              "        ...,\n",
              "        [0.25622875, 0.08116025, 0.06246725, 0.        , 0.19020918],\n",
              "        [0.08024779, 0.28263701, 0.37400094, 0.46074731, 0.26354338],\n",
              "        [0.50601461, 0.3491967 , 0.54022084, 0.41885347, 0.        ]]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_test_all).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmwtXvCvlOyY",
        "outputId": "54b001c6-9484-4435-c3e0-d247eaa2e385"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    11266\n",
              "0.0    10395\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZKxcbnfnfZJ",
        "outputId": "9e16a1fd-4e83-4d72-d809-aefd407ed832"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(212887, 128, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Time2Vector(Layer):\n",
        "  def __init__(self, seq_len, **kwargs):\n",
        "    super(Time2Vector, self).__init__()\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    '''Initialize weights and biases with shape (batch, seq_len)'''\n",
        "    self.weights_linear = self.add_weight(name='weight_linear',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "    \n",
        "    self.bias_linear = self.add_weight(name='bias_linear',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "    \n",
        "    self.weights_periodic = self.add_weight(name='weight_periodic',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "    self.bias_periodic = self.add_weight(name='bias_periodic',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    '''Calculate linear and periodic time features'''\n",
        "    x = tf.math.reduce_mean(x[:,:,:4], axis=-1) \n",
        "    time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
        "    time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
        "    \n",
        "    time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
        "    time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
        "    return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
        "   \n",
        "  def get_config(self): # Needed for saving and loading model with custom layer\n",
        "    config = super().get_config().copy()\n",
        "    config.update({'seq_len': self.seq_len})\n",
        "    return config"
      ],
      "metadata": {
        "id": "7re6APIlcnrc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleAttention(Layer):\n",
        "  def __init__(self, d_k, d_v):\n",
        "    super(SingleAttention, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.query = Dense(self.d_k, \n",
        "                       input_shape=input_shape, \n",
        "                       kernel_initializer='glorot_uniform', \n",
        "                       bias_initializer='glorot_uniform')\n",
        "    \n",
        "    self.key = Dense(self.d_k, \n",
        "                     input_shape=input_shape, \n",
        "                     kernel_initializer='glorot_uniform', \n",
        "                     bias_initializer='glorot_uniform')\n",
        "    \n",
        "    self.value = Dense(self.d_v, \n",
        "                       input_shape=input_shape, \n",
        "                       kernel_initializer='glorot_uniform', \n",
        "                       bias_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
        "    q = self.query(inputs[0])\n",
        "    k = self.key(inputs[1])\n",
        "\n",
        "    attn_weights = tf.matmul(q, k, transpose_b=True)\n",
        "    attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
        "    attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
        "    \n",
        "    v = self.value(inputs[2])\n",
        "    attn_out = tf.matmul(attn_weights, v)\n",
        "    return attn_out    \n",
        "\n",
        "#############################################################################\n",
        "\n",
        "class MultiAttention(Layer):\n",
        "  def __init__(self, d_k, d_v, n_heads):\n",
        "    super(MultiAttention, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.n_heads = n_heads\n",
        "    self.attn_heads = list()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    for n in range(self.n_heads):\n",
        "      self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
        "    \n",
        "    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n",
        "    self.linear = Dense(input_shape[0][-1], \n",
        "                        input_shape=input_shape, \n",
        "                        kernel_initializer='glorot_uniform', \n",
        "                        bias_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
        "    concat_attn = tf.concat(attn, axis=-1)\n",
        "    multi_linear = self.linear(concat_attn)\n",
        "    return multi_linear   \n",
        "\n",
        "#############################################################################\n",
        "\n",
        "class TransformerEncoder(Layer):\n",
        "  def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.n_heads = n_heads\n",
        "    self.ff_dim = ff_dim\n",
        "    self.attn_heads = list()\n",
        "    self.dropout_rate = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
        "    self.attn_dropout = Dropout(self.dropout_rate)\n",
        "    self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
        "\n",
        "    self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
        "    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 \n",
        "    self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n",
        "    self.ff_dropout = Dropout(self.dropout_rate)\n",
        "    self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n",
        "  \n",
        "  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
        "    attn_layer = self.attn_multi(inputs)\n",
        "    attn_layer = self.attn_dropout(attn_layer)\n",
        "    attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
        "\n",
        "    ff_layer = self.ff_conv1D_1(attn_layer)\n",
        "    ff_layer = self.ff_conv1D_2(ff_layer)\n",
        "    ff_layer = self.ff_dropout(ff_layer)\n",
        "    ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
        "    return ff_layer \n",
        "\n",
        "  def get_config(self): # Needed for saving and loading model with custom layer\n",
        "    config = super().get_config().copy()\n",
        "    config.update({'d_k': self.d_k,\n",
        "                   'd_v': self.d_v,\n",
        "                   'n_heads': self.n_heads,\n",
        "                   'ff_dim': self.ff_dim,\n",
        "                   'attn_heads': self.attn_heads,\n",
        "                   'dropout_rate': self.dropout_rate})\n",
        "    return config          "
      ],
      "metadata": {
        "id": "7urkpDI_ctUG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  '''Initialize time and transformer layers'''\n",
        "  time_embedding = Time2Vector(seq_len)\n",
        "  attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "\n",
        "  '''Construct model'''\n",
        "  in_seq = Input(shape=(seq_len, 5))\n",
        "  x = time_embedding(in_seq)\n",
        "  x = Concatenate(axis=-1)([in_seq, x])\n",
        "  x = attn_layer1((x, x, x))\n",
        "  x = attn_layer2((x, x, x))\n",
        "  x = attn_layer3((x, x, x))\n",
        "  x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  x = Dense(64, activation='relu')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs=in_seq, outputs=out)\n",
        "  model.compile(loss='mse', optimizer='adam', metrics=['mae', 'accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "k3X3H76yc4Wz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=.80)\n",
        "sess=tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
        "model = create_model()\n",
        "model.summary()\n",
        "\n",
        "NAME=f\"StockTranformers{datetime.datetime.now().strftime('%m%d%y')}\"\n",
        "\n",
        "log_dir = f'./logs/{NAME}'\n",
        "tensorboard=tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "callback = tf.keras.callbacks.ModelCheckpoint('Transformer+TimeEmbedding.hdf5', \n",
        "                                              monitor='val_loss', \n",
        "                                              save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YeCC8iRc_il",
        "outputId": "edcc0088-fb6d-4e74-b69a-7dd71dac9749"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 5)]     0           []                               \n",
            "                                                                                                  \n",
            " time2_vector (Time2Vector)     (None, 128, 2)       512         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 128, 7)       0           ['input_1[0][0]',                \n",
            "                                                                  'time2_vector[0][0]']           \n",
            "                                                                                                  \n",
            " transformer_encoder (Transform  (None, 128, 7)      99114       ['concatenate[0][0]',            \n",
            " erEncoder)                                                       'concatenate[0][0]',            \n",
            "                                                                  'concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Transfo  (None, 128, 7)      99114       ['transformer_encoder[0][0]',    \n",
            " rmerEncoder)                                                     'transformer_encoder[0][0]',    \n",
            "                                                                  'transformer_encoder[0][0]']    \n",
            "                                                                                                  \n",
            " transformer_encoder_2 (Transfo  (None, 128, 7)      99114       ['transformer_encoder_1[0][0]',  \n",
            " rmerEncoder)                                                     'transformer_encoder_1[0][0]',  \n",
            "                                                                  'transformer_encoder_1[0][0]']  \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 128)         0           ['transformer_encoder_2[0][0]']  \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 128)          0           ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 64)           8256        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 64)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            65          ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 306,175\n",
            "Trainable params: 306,175\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "import numpy as np   \n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return batch_x, batch_y\n",
        "\n",
        "train_gen = DataGenerator(X_train_all, y_train_all, 32)\n",
        "val_gen = DataGenerator(X_val_all, y_val_all, 32)"
      ],
      "metadata": {
        "id": "rUx2QvTTcv6s"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not LOAD_MODEL:\n",
        "  history = model.fit(train_gen,  \n",
        "                      epochs=2, \n",
        "                      callbacks=[callback,tensorboard],\n",
        "                      validation_data=val_gen)  \n",
        "\n",
        "  model.save('stock_pred_transformers')"
      ],
      "metadata": {
        "id": "Ctw4WjLHdXLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c957a50f-aba5-4c7d-f869-6211708f1ad6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "6653/6653 [==============================] - ETA: 0s - loss: 0.2456 - mae: 0.4911 - accuracy: 0.5673\n",
            "Epoch 1: val_loss improved from inf to 0.24979, saving model to Transformer+TimeEmbedding.hdf5\n",
            "6653/6653 [==============================] - 16380s 2s/step - loss: 0.2456 - mae: 0.4911 - accuracy: 0.5673 - val_loss: 0.2498 - val_mae: 0.4964 - val_accuracy: 0.5310\n",
            "Epoch 2/2\n",
            "6653/6653 [==============================] - ETA: 0s - loss: 0.2456 - mae: 0.4910 - accuracy: 0.5674\n",
            "Epoch 2: val_loss did not improve from 0.24979\n",
            "6653/6653 [==============================] - 16421s 2s/step - loss: 0.2456 - mae: 0.4910 - accuracy: 0.5674 - val_loss: 0.2502 - val_mae: 0.4959 - val_accuracy: 0.5310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_attention_layer_call_fn, multi_attention_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn while saving (showing 5 of 336). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: stock_pred_transformers/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: stock_pred_transformers/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('multitock_classification_change.hdf5',\n",
        "                                   custom_objects={'Time2Vector': Time2Vector, \n",
        "                                                   'SingleAttention': SingleAttention,\n",
        "                                                   'MultiAttention': MultiAttention,\n",
        "                                                   'TransformerEncoder': TransformerEncoder})"
      ],
      "metadata": {
        "id": "NXhYjabHdcnE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate predication for training, validation and test data\n",
        "# train_pred = model.predict(X_train_all)\n",
        "# val_pred = model.predict(X_val_all)\n",
        "# test_pred = model.predict(X_test_all)\n",
        "\n",
        "# #Print evaluation metrics for all datasets\n",
        "# train_eval = model.evaluate(X_train_all, y_train_all, verbose=0)\n",
        "# val_eval = model.evaluate(X_val_all, y_val_all, verbose=0)\n",
        "# test_eval = model.evaluate(X_test_all, y_test_all, verbose=0)\n",
        "\n",
        "# print(' ')\n",
        "# print('Evaluation metrics')\n",
        "# print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval[0], train_eval[1], train_eval[2]))\n",
        "# print('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval[0], val_eval[1], val_eval[2]))\n",
        "# print('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval[0], test_eval[1], test_eval[2]))"
      ],
      "metadata": {
        "id": "xSzJ--3TdjHm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_data(ticker='IBM'):\n",
        "   df_ibm = yf.download(ticker).reset_index()\n",
        "   df_ibm = df_ibm.drop(columns='Close')\n",
        "   df_ibm.columns=['Date','Open','High','Low','Close','Volume']\n",
        "   train_data,val_data,test_data,min_max = preprocessing(df_ibm)\n",
        "   X_train,y_train,X_val,y_val,X_test,y_test = train_test_split(train_data,val_data,test_data)\n",
        "   return (X_test,y_test,min_max)"
      ],
      "metadata": {
        "id": "wrOzFO5odtgd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_X_test,my_y_test,my_min_max = get_test_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acUxPzaeeRmt",
        "outputId": "ac6284b2-689b-47d4-d29a-5243df864e2d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (12184, 6)\n",
            "Validation data shape: (1523, 6)\n",
            "Test data shape: (1523, 6)\n",
            "Training set shape (12056, 128, 5) (12056,)\n",
            "Validation set shape (1395, 128, 5) (1395,)\n",
            "Testing set shape (1395, 128, 5) (1395,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_pred = model.predict(my_X_test)"
      ],
      "metadata": {
        "id": "vEmvw60PeitO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(my_pred<=0.5).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad67apejO6Bc",
        "outputId": "19974785-8c15-4bb0-9d61-5f5d006ca93e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_pred.flatten().tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_gGmpzOPtKT",
        "outputId": "b2271e29-3d4d-4655-bff4-d6f843e36bf2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5584424734115601,\n",
              " 0.5584431290626526,\n",
              " 0.5584422945976257,\n",
              " 0.5584429502487183,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584424734115601,\n",
              " 0.5584430694580078,\n",
              " 0.5584425926208496,\n",
              " 0.5584425330162048,\n",
              " 0.5584429502487183,\n",
              " 0.5584424734115601,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584425926208496,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584423542022705,\n",
              " 0.5584429502487183,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.5584424734115601,\n",
              " 0.558443009853363,\n",
              " 0.5584425926208496,\n",
              " 0.5584425330162048,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584431886672974,\n",
              " 0.5584424138069153,\n",
              " 0.5584428906440735,\n",
              " 0.5584426522254944,\n",
              " 0.5584424734115601,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584425330162048,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.558443009853363,\n",
              " 0.5584424734115601,\n",
              " 0.5584429502487183,\n",
              " 0.5584425330162048,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584428906440735,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584430694580078,\n",
              " 0.5584422945976257,\n",
              " 0.5584426522254944,\n",
              " 0.558443009853363,\n",
              " 0.5584424138069153,\n",
              " 0.5584427118301392,\n",
              " 0.5584429502487183,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584425330162048,\n",
              " 0.5584428906440735,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584424138069153,\n",
              " 0.5584428906440735,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584425330162048,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584429502487183,\n",
              " 0.5584424734115601,\n",
              " 0.5584426522254944,\n",
              " 0.5584428906440735,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584424138069153,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584424734115601,\n",
              " 0.5584429502487183,\n",
              " 0.5584425926208496,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584428906440735,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584428906440735,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584424734115601,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584429502487183,\n",
              " 0.5584424734115601,\n",
              " 0.5584426522254944,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584429502487183,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584428906440735,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584425330162048,\n",
              " 0.5584426522254944,\n",
              " 0.5584425330162048,\n",
              " 0.5584428906440735,\n",
              " 0.5584428906440735,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584425330162048,\n",
              " 0.5584428906440735,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584425330162048,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584429502487183,\n",
              " 0.5584424734115601,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584425330162048,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584424734115601,\n",
              " 0.5584427714347839,\n",
              " 0.5584428906440735,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584425926208496,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.5584431886672974,\n",
              " 0.5584425330162048,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584425926208496,\n",
              " 0.5584425926208496,\n",
              " 0.5584428906440735,\n",
              " 0.5584427118301392,\n",
              " 0.5584425330162048,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584428906440735,\n",
              " 0.5584424734115601,\n",
              " 0.5584428906440735,\n",
              " 0.5584429502487183,\n",
              " 0.5584427714347839,\n",
              " 0.5584424734115601,\n",
              " 0.5584427118301392,\n",
              " 0.5584424734115601,\n",
              " 0.5584429502487183,\n",
              " 0.5584425926208496,\n",
              " 0.558443009853363,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584424138069153,\n",
              " 0.5584429502487183,\n",
              " 0.558443009853363,\n",
              " 0.558442234992981,\n",
              " 0.558443009853363,\n",
              " 0.5584428310394287,\n",
              " 0.5584424734115601,\n",
              " 0.5584425926208496,\n",
              " 0.5584428906440735,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584422945976257,\n",
              " 0.5584428906440735,\n",
              " 0.5584426522254944,\n",
              " 0.5584425330162048,\n",
              " 0.5584428906440735,\n",
              " 0.5584428906440735,\n",
              " 0.5584426522254944,\n",
              " 0.5584424734115601,\n",
              " 0.5584428310394287,\n",
              " 0.5584428906440735,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584428310394287,\n",
              " 0.5584428906440735,\n",
              " 0.5584424138069153,\n",
              " 0.5584428310394287,\n",
              " 0.5584428906440735,\n",
              " 0.5584423542022705,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584429502487183,\n",
              " 0.5584422945976257,\n",
              " 0.558443009853363,\n",
              " 0.5584428310394287,\n",
              " 0.5584427714347839,\n",
              " 0.5584425330162048,\n",
              " 0.5584430694580078,\n",
              " 0.5584425330162048,\n",
              " 0.5584428906440735,\n",
              " 0.5584425330162048,\n",
              " 0.5584425926208496,\n",
              " 0.5584429502487183,\n",
              " 0.5584428310394287,\n",
              " 0.5584424138069153,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584427714347839,\n",
              " 0.5584425330162048,\n",
              " 0.5584427118301392,\n",
              " 0.558443009853363,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584423542022705,\n",
              " 0.5584427118301392,\n",
              " 0.558443009853363,\n",
              " 0.5584424734115601,\n",
              " 0.5584428310394287,\n",
              " 0.5584424734115601,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584424138069153,\n",
              " 0.5584427714347839,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584424734115601,\n",
              " 0.5584428906440735,\n",
              " 0.5584427118301392,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584424734115601,\n",
              " 0.5584427118301392,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.558443009853363,\n",
              " 0.5584425330162048,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584429502487183,\n",
              " 0.5584425330162048,\n",
              " 0.5584426522254944,\n",
              " 0.5584429502487183,\n",
              " 0.5584422945976257,\n",
              " 0.5584430694580078,\n",
              " 0.5584425330162048,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584424734115601,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584429502487183,\n",
              " 0.5584426522254944,\n",
              " 0.5584425926208496,\n",
              " 0.5584424734115601,\n",
              " 0.558443009853363,\n",
              " 0.5584426522254944,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584424734115601,\n",
              " 0.558443009853363,\n",
              " 0.5584424734115601,\n",
              " 0.5584428906440735,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584424138069153,\n",
              " 0.5584427714347839,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584429502487183,\n",
              " 0.5584426522254944,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584425330162048,\n",
              " 0.5584429502487183,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584428906440735,\n",
              " 0.5584426522254944,\n",
              " 0.558443009853363,\n",
              " 0.5584425330162048,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584429502487183,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584429502487183,\n",
              " 0.5584423542022705,\n",
              " 0.5584428310394287,\n",
              " 0.558443009853363,\n",
              " 0.5584427118301392,\n",
              " 0.5584424734115601,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584429502487183,\n",
              " 0.5584424734115601,\n",
              " 0.5584427714347839,\n",
              " 0.5584423542022705,\n",
              " 0.5584427118301392,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584429502487183,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584424734115601,\n",
              " 0.5584431886672974,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.558443009853363,\n",
              " 0.5584428906440735,\n",
              " 0.5584424734115601,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584429502487183,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584424734115601,\n",
              " 0.5584428310394287,\n",
              " 0.5584428906440735,\n",
              " 0.5584425330162048,\n",
              " 0.5584425330162048,\n",
              " 0.5584427714347839,\n",
              " 0.5584428906440735,\n",
              " 0.5584424734115601,\n",
              " 0.5584426522254944,\n",
              " 0.558442234992981,\n",
              " 0.5584430694580078,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584425330162048,\n",
              " 0.5584422945976257,\n",
              " 0.5584429502487183,\n",
              " 0.5584427714347839,\n",
              " 0.5584424734115601,\n",
              " 0.5584425330162048,\n",
              " 0.5584432482719421,\n",
              " 0.5584425926208496,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584428906440735,\n",
              " 0.5584427714347839,\n",
              " 0.5584429502487183,\n",
              " 0.5584422945976257,\n",
              " 0.5584429502487183,\n",
              " 0.5584428310394287,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584425330162048,\n",
              " 0.5584430694580078,\n",
              " 0.5584427118301392,\n",
              " 0.5584424138069153,\n",
              " 0.5584429502487183,\n",
              " 0.5584424138069153,\n",
              " 0.5584427714347839,\n",
              " 0.558443009853363,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584425330162048,\n",
              " 0.5584426522254944,\n",
              " 0.558443009853363,\n",
              " 0.5584421157836914,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584424734115601,\n",
              " 0.5584428310394287,\n",
              " 0.5584425330162048,\n",
              " 0.5584425926208496,\n",
              " 0.5584425926208496,\n",
              " 0.558443009853363,\n",
              " 0.5584424734115601,\n",
              " 0.5584426522254944,\n",
              " 0.5584424138069153,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584424734115601,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584428906440735,\n",
              " 0.5584425330162048,\n",
              " 0.5584429502487183,\n",
              " 0.5584425926208496,\n",
              " 0.5584425330162048,\n",
              " 0.5584430694580078,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584425330162048,\n",
              " 0.5584427118301392,\n",
              " 0.5584430694580078,\n",
              " 0.5584423542022705,\n",
              " 0.5584428906440735,\n",
              " 0.5584427118301392,\n",
              " 0.5584424734115601,\n",
              " 0.5584430694580078,\n",
              " 0.5584425330162048,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584423542022705,\n",
              " 0.5584428906440735,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584424734115601,\n",
              " 0.5584429502487183,\n",
              " 0.5584426522254944,\n",
              " 0.5584428906440735,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584428906440735,\n",
              " 0.5584427118301392,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584429502487183,\n",
              " 0.5584427714347839,\n",
              " 0.5584424734115601,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.558443009853363,\n",
              " 0.5584424734115601,\n",
              " 0.5584429502487183,\n",
              " 0.5584424734115601,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584424734115601,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.558443009853363,\n",
              " 0.5584426522254944,\n",
              " 0.5584424138069153,\n",
              " 0.5584429502487183,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584428310394287,\n",
              " 0.5584423542022705,\n",
              " 0.5584427714347839,\n",
              " 0.5584425330162048,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584424138069153,\n",
              " 0.5584427118301392,\n",
              " 0.5584427118301392,\n",
              " 0.5584432482719421,\n",
              " 0.5584427118301392,\n",
              " 0.5584423542022705,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584428906440735,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584425330162048,\n",
              " 0.5584429502487183,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.5584425926208496,\n",
              " 0.5584425330162048,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.5584423542022705,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.558443009853363,\n",
              " 0.5584424734115601,\n",
              " 0.5584424138069153,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584429502487183,\n",
              " 0.5584427118301392,\n",
              " 0.5584424734115601,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584430694580078,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.558443009853363,\n",
              " 0.5584425330162048,\n",
              " 0.5584425330162048,\n",
              " 0.5584428906440735,\n",
              " 0.5584425926208496,\n",
              " 0.558443009853363,\n",
              " 0.5584427714347839,\n",
              " 0.5584428310394287,\n",
              " 0.5584425330162048,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584425926208496,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.5584430694580078,\n",
              " 0.5584424734115601,\n",
              " 0.5584424734115601,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584430694580078,\n",
              " 0.5584425926208496,\n",
              " 0.5584424734115601,\n",
              " 0.558443009853363,\n",
              " 0.5584425926208496,\n",
              " 0.5584424734115601,\n",
              " 0.5584428906440735,\n",
              " 0.5584427714347839,\n",
              " 0.5584429502487183,\n",
              " 0.5584424734115601,\n",
              " 0.5584429502487183,\n",
              " 0.5584425330162048,\n",
              " 0.5584426522254944,\n",
              " 0.5584429502487183,\n",
              " 0.5584424138069153,\n",
              " 0.5584429502487183,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584428906440735,\n",
              " 0.5584429502487183,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.558443009853363,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.558443009853363,\n",
              " 0.5584425330162048,\n",
              " 0.5584428310394287,\n",
              " 0.5584425330162048,\n",
              " 0.5584424734115601,\n",
              " 0.5584428310394287,\n",
              " 0.558443009853363,\n",
              " 0.5584424138069153,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584425330162048,\n",
              " 0.5584425330162048,\n",
              " 0.5584430694580078,\n",
              " 0.5584424734115601,\n",
              " 0.5584425330162048,\n",
              " 0.5584431290626526,\n",
              " 0.5584421157836914,\n",
              " 0.558443009853363,\n",
              " 0.5584425330162048,\n",
              " 0.5584428906440735,\n",
              " 0.5584423542022705,\n",
              " 0.5584425330162048,\n",
              " 0.5584426522254944,\n",
              " 0.5584426522254944,\n",
              " 0.5584431886672974,\n",
              " 0.5584423542022705,\n",
              " 0.5584433674812317,\n",
              " 0.5584426522254944,\n",
              " 0.5584421157836914,\n",
              " 0.5584434866905212,\n",
              " 0.5584425926208496,\n",
              " 0.5584429502487183,\n",
              " 0.5584432482719421,\n",
              " 0.5584425330162048,\n",
              " 0.5584427118301392,\n",
              " 0.5584431290626526,\n",
              " 0.5584428310394287,\n",
              " 0.5584427118301392,\n",
              " 0.5584433078765869,\n",
              " 0.5584426522254944,\n",
              " 0.5584425926208496,\n",
              " 0.558443009853363,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584424734115601,\n",
              " 0.5584428310394287,\n",
              " 0.5584424138069153,\n",
              " 0.5584428310394287,\n",
              " 0.5584429502487183,\n",
              " 0.5584424138069153,\n",
              " 0.5584429502487183,\n",
              " 0.5584421157836914,\n",
              " 0.5584431886672974,\n",
              " 0.5584425926208496,\n",
              " 0.5584423542022705,\n",
              " 0.5584424734115601,\n",
              " 0.5584425926208496,\n",
              " 0.5584425926208496,\n",
              " 0.5584419369697571,\n",
              " 0.5584434270858765,\n",
              " 0.5584418773651123,\n",
              " 0.5584432482719421,\n",
              " 0.5584425926208496,\n",
              " 0.5584421753883362,\n",
              " 0.5584425330162048,\n",
              " 0.5584430694580078,\n",
              " 0.5584431886672974,\n",
              " 0.5584418177604675,\n",
              " 0.5584433078765869,\n",
              " 0.5584423542022705,\n",
              " 0.5584428310394287,\n",
              " 0.5584432482719421,\n",
              " 0.5584424138069153,\n",
              " 0.5584433078765869,\n",
              " 0.5584427118301392,\n",
              " 0.5584425330162048,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584424138069153,\n",
              " 0.5584431886672974,\n",
              " 0.5584431290626526,\n",
              " 0.5584423542022705,\n",
              " 0.5584433078765869,\n",
              " 0.558442234992981,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584431290626526,\n",
              " 0.5584424734115601,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584421753883362,\n",
              " 0.5584428906440735,\n",
              " 0.5584424734115601,\n",
              " 0.5584426522254944,\n",
              " 0.5584425330162048,\n",
              " 0.5584431886672974,\n",
              " 0.5584428310394287,\n",
              " 0.5584418773651123,\n",
              " 0.558443546295166,\n",
              " 0.5584418177604675,\n",
              " 0.5584433674812317,\n",
              " 0.5584420561790466,\n",
              " 0.5584427714347839,\n",
              " 0.558442234992981,\n",
              " 0.5584428310394287,\n",
              " 0.5584428310394287,\n",
              " 0.5584423542022705,\n",
              " 0.5584433078765869,\n",
              " 0.5584421157836914,\n",
              " 0.5584428310394287,\n",
              " 0.5584425926208496,\n",
              " 0.5584425330162048,\n",
              " 0.5584428906440735,\n",
              " 0.5584428310394287,\n",
              " 0.5584429502487183,\n",
              " 0.5584421157836914,\n",
              " 0.5584431886672974,\n",
              " 0.5584420561790466,\n",
              " 0.5584429502487183,\n",
              " 0.5584431290626526,\n",
              " 0.5584420561790466,\n",
              " 0.5584433078765869,\n",
              " 0.5584423542022705,\n",
              " 0.558443009853363,\n",
              " 0.5584419369697571,\n",
              " 0.5584431290626526,\n",
              " 0.5584426522254944,\n",
              " 0.5584427714347839,\n",
              " 0.5584428310394287,\n",
              " 0.5584423542022705,\n",
              " 0.5584429502487183,\n",
              " 0.5584428310394287,\n",
              " 0.5584427714347839,\n",
              " 0.5584423542022705,\n",
              " 0.5584426522254944,\n",
              " 0.5584429502487183,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584425926208496,\n",
              " 0.558442234992981,\n",
              " 0.5584433078765869,\n",
              " 0.5584425926208496,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584425330162048,\n",
              " 0.5584426522254944,\n",
              " 0.5584431886672974,\n",
              " 0.5584423542022705,\n",
              " 0.5584428906440735,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584426522254944,\n",
              " 0.5584429502487183,\n",
              " 0.5584421753883362,\n",
              " 0.5584430694580078,\n",
              " 0.5584427714347839,\n",
              " 0.5584427118301392,\n",
              " 0.5584428310394287,\n",
              " 0.5584424734115601,\n",
              " 0.5584428310394287,\n",
              " 0.5584426522254944,\n",
              " 0.5584434866905212,\n",
              " 0.5584424138069153,\n",
              " 0.5584426522254944,\n",
              " 0.5584425926208496,\n",
              " 0.5584428906440735,\n",
              " 0.5584424734115601,\n",
              " 0.5584431290626526,\n",
              " 0.5584424734115601,\n",
              " 0.5584427714347839,\n",
              " 0.5584430694580078,\n",
              " 0.5584425330162048,\n",
              " 0.5584423542022705,\n",
              " 0.5584429502487183,\n",
              " 0.5584423542022705,\n",
              " 0.5584427714347839,\n",
              " 0.5584426522254944,\n",
              " 0.5584427118301392,\n",
              " 0.5584427714347839,\n",
              " 0.5584425926208496,\n",
              " 0.5584424138069153,\n",
              " 0.5584424734115601,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = my_pred.copy()\n",
        "pred[pred>=0.5]=1\n",
        "pred[pred<0.5]=0\n",
        "(pred.flatten()==my_y_test).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zybfOGNEapt",
        "outputId": "94a698a4-3537-4062-e0ee-033c1a27cca8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5225806451612903"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "pd.Series(my_pred.flatten()).plot()\n",
        "plt.axhline(y=0.5,color='red')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "1HLCZy-sEZ8m",
        "outputId": "e45bc0eb-41bf-477c-aa56-6435318b1e34"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7fce56619390>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def decode_from_norm(std_change,min_max):\n",
        "#   min, max = min_max\n",
        "#   return std_change*(max-min)+min"
      ],
      "metadata": {
        "id": "eMOf5eXaepMA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(15,10))\n",
        "# my_pred_dec = decode_from_norm(my_pred,my_min_max)\n",
        "# my_y_test_dec = decode_from_norm(my_y_test,my_min_max)\n",
        "# pd.Series(my_y_test_dec).plot(label=\"True percentage change!\")\n",
        "# pd.Series(my_pred_dec.flatten()).plot(label=\"Predicted percentage change!\")\n",
        "# plt.axhline(y=0,color='red')\n",
        "# plt.title(\"Stock Adj.Close predicted change!\")\n",
        "# plt.legend()"
      ],
      "metadata": {
        "id": "sUJyxJKVfZ9w"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IvApJguUETvK"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}
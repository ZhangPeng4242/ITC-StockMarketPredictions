{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock_Prediction_Transformers_multistocks_classification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "771bc0b91bfa424798feecbbdf509f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee0f8d07ebc1464b85addc9f7bacb38b",
              "IPY_MODEL_59884878cdfe4609af02f3d53ab41a32",
              "IPY_MODEL_26b2f20063864509980435ba26ca1dfd"
            ],
            "layout": "IPY_MODEL_335283379e8a4b41a326b6fcc9ce813d"
          }
        },
        "ee0f8d07ebc1464b85addc9f7bacb38b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09d359e76459438292279e9b3717e35a",
            "placeholder": "​",
            "style": "IPY_MODEL_2aa12790fcea47a19de5f7524ef38ad9",
            "value": "100%"
          }
        },
        "59884878cdfe4609af02f3d53ab41a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b76fe5af46a9439ea0d20fbda26324cd",
            "max": 55,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24ed0717a7994c2dbac90893c56a3d08",
            "value": 55
          }
        },
        "26b2f20063864509980435ba26ca1dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99f4348ec77b4c37aa1def60592eaa7f",
            "placeholder": "​",
            "style": "IPY_MODEL_bd8f780d84784eb9b18bd43fb9fddc6f",
            "value": " 55/55 [00:35&lt;00:00,  1.06it/s]"
          }
        },
        "335283379e8a4b41a326b6fcc9ce813d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d359e76459438292279e9b3717e35a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aa12790fcea47a19de5f7524ef38ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b76fe5af46a9439ea0d20fbda26324cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24ed0717a7994c2dbac90893c56a3d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99f4348ec77b4c37aa1def60592eaa7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd8f780d84784eb9b18bd43fb9fddc6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This is to control wether we load trained model or train the model from scratch\n",
        "LOAD_MODEL = False"
      ],
      "metadata": {
        "id": "ZMoDRLcDe9aR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDc4wZema-Ca",
        "outputId": "15029ea8-d04e-4ad6-f3c0-4b3eaefd4c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.72)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.28.1)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.9.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Tensorflow version: 2.8.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, datetime\n",
        "!pip install yfinance\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "print('Tensorflow version: {}'.format(tf.__version__))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(df):\n",
        "  # '''Calculate percentage change'''\n",
        "  # df['Open'] = df['Open'].pct_change() # Create arithmetic returns column\n",
        "  # df['High'] = df['High'].pct_change() # Create arithmetic returns column\n",
        "  # df['Low'] = df['Low'].pct_change() # Create arithmetic returns column\n",
        "  # df['Close'] = df['Close'].pct_change() # Create arithmetic returns column\n",
        "  # df['Volume'] = df['Volume'].pct_change()\n",
        "  #  df['Close'] = df['Close'].diff()\n",
        "  # df.dropna(how='any', axis=0, inplace=True) # Drop all rows with NaN values\n",
        "  # df.loc[df['Close']>=0,'Close']=1\n",
        "  # df.loc[df['Close']<0,'Close']=0\n",
        "  df['diff']=df['Close'].diff()\n",
        "  df.dropna(how='any', axis=0, inplace=True) # Drop all rows with NaN values\n",
        "\n",
        "  ###############################################################################\n",
        "  '''Create indexes to split dataset'''\n",
        "\n",
        "  times = sorted(df.index.values)\n",
        "  last_10pct = sorted(df.index.values)[-int(0.1*len(times))] # Last 10% of series\n",
        "  last_20pct = sorted(df.index.values)[-int(0.2*len(times))] # Last 20% of series\n",
        "\n",
        "  ###############################################################################\n",
        "  '''Normalize price columns'''\n",
        "  \n",
        "  df = df.dropna()\n",
        "  df = df[df!=np.inf]\n",
        "  min_return = min(df[(df.index < last_20pct)][['Open', 'High', 'Low', 'Close']].min(axis=0))\n",
        "  max_return = max(df[(df.index < last_20pct)][['Open', 'High', 'Low', 'Close']].max(axis=0))\n",
        "\n",
        "  # Min-max normalize price columns (0-1 range)\n",
        "  df['Open'] = (df['Open'] - min_return) / (max_return - min_return)\n",
        "  df['High'] = (df['High'] - min_return) / (max_return - min_return)\n",
        "  df['Low'] = (df['Low'] - min_return) / (max_return - min_return)\n",
        "  df['Close'] = (df['Close'] - min_return) / (max_return - min_return)\n",
        "\n",
        "  ###############################################################################\n",
        "  '''Normalize volume column'''\n",
        "  df = df.dropna()\n",
        "  df = df[df!=np.inf]\n",
        "  min_volume = df[(df.index < last_20pct)]['Volume'].min(axis=0)\n",
        "  max_volume = df[(df.index < last_20pct)]['Volume'].max(axis=0)\n",
        "\n",
        "  # Min-max normalize volume columns (0-1 range)\n",
        "  df['Volume'] = (df['Volume'] - min_volume) / (max_volume - min_volume)\n",
        "\n",
        "  ###############################################################################\n",
        "  '''Create training, validation and test split'''\n",
        "\n",
        "  df = df.dropna()\n",
        "  df = df[df!=np.inf]\n",
        "  df_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\n",
        "  df_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\n",
        "  df_test = df[(df.index >= last_10pct)]\n",
        "\n",
        "  # Remove date column\n",
        "  df_train.drop(columns=['Date'], inplace=True)\n",
        "  df_val.drop(columns=['Date'], inplace=True)\n",
        "  df_test.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "  # Convert pandas columns into arrays\n",
        "  train_data = df_train.values\n",
        "  val_data = df_val.values\n",
        "  test_data = df_test.values\n",
        "\n",
        "  print('Training data shape: {}'.format(train_data.shape))\n",
        "  print('Validation data shape: {}'.format(val_data.shape))\n",
        "  print('Test data shape: {}'.format(test_data.shape))\n",
        "  return (train_data,val_data,test_data,(min_return,max_return))"
      ],
      "metadata": {
        "id": "nMFPFDmtbcVl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(train_data,val_data,test_data):\n",
        "  # Training data\n",
        "  X_train, y_train = [], []\n",
        "  for i in range(seq_len, len(train_data)):\n",
        "    X_train.append(train_data[i-seq_len:i]) # Chunks of training data with a length of 128 df-rows\n",
        "    y_train.append(train_data[:, 5][i]) #Value of 4th column (Close Price) of df-row 128+1\n",
        "  X_train, y_train = np.array(X_train)[:,:,:5], np.array(y_train)\n",
        "  y_train[y_train>=0]=1\n",
        "  y_train[y_train<0]=0\n",
        "  ###############################################################################\n",
        "\n",
        "  # Validation data\n",
        "  X_val, y_val = [], []\n",
        "  for i in range(seq_len, len(val_data)):\n",
        "      X_val.append(val_data[i-seq_len:i])\n",
        "      y_val.append(val_data[:, 5][i])\n",
        "  X_val, y_val = np.array(X_val)[:,:,:5], np.array(y_val)\n",
        "  y_val[y_val>=0]=1\n",
        "  y_val[y_val<0]=0\n",
        "  ###############################################################################\n",
        "\n",
        "  # Test data\n",
        "  X_test, y_test = [], []\n",
        "  for i in range(seq_len, len(test_data)):\n",
        "      X_test.append(test_data[i-seq_len:i])\n",
        "      y_test.append(test_data[:, 5][i])    \n",
        "  X_test, y_test = np.array(X_test)[:,:,:5], np.array(y_test)\n",
        "  y_test[y_test>=0]=1\n",
        "  y_test[y_test<0]=0\n",
        "  print('Training set shape', X_train.shape, y_train.shape)\n",
        "  print('Validation set shape', X_val.shape, y_val.shape)\n",
        "  print('Testing set shape' ,X_test.shape, y_test.shape)\n",
        "  return (X_train,y_train,X_val,y_val,X_test,y_test)"
      ],
      "metadata": {
        "id": "WVcUU45-cP9S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "seq_len = 128\n",
        "\n",
        "d_k = 256\n",
        "d_v = 256\n",
        "n_heads = 12\n",
        "ff_dim = 256"
      ],
      "metadata": {
        "id": "MkALJPyCbIlG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/tickers.txt') as tfile:\n",
        "  tickers = tfile.read()\n",
        "tickers = tickers.split()"
      ],
      "metadata": {
        "id": "zpq2QFpObJxB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combining different stocks to train, val, test set\n",
        "np.random.seed(42)\n",
        "data_history = []\n",
        "first=True\n",
        "for ticker in tqdm(np.random.choice(tickers,size=50, replace=False).tolist()+['AAPL','IBM','QQQ','NVDA','TSLA'],total=55):\n",
        "  print(ticker)\n",
        "  df = yf.download(ticker).reset_index()\n",
        "  df = df.drop(columns='Close')\n",
        "  df.columns=['Date','Open','High','Low','Close','Volume']\n",
        "  if len(df.index)==0:\n",
        "    print(\"Cannot featch the data! Ticker not found.\")\n",
        "    continue\n",
        "  if len(df.index)<=seq_len*10:\n",
        "    print(\"Stock Data Not Big Enough!\")\n",
        "    continue\n",
        "    \n",
        "  train_data,val_data,test_data,min_max = preprocessing(df)\n",
        "  data_history.append((ticker,min_max,(len(train_data),len(val_data),len(test_data))))\n",
        "  X_train,y_train,X_val,y_val,X_test,y_test = train_test_split(train_data,val_data,test_data)\n",
        "  if first:\n",
        "    X_train_all,y_train_all,X_val_all,y_val_all,X_test_all,y_test_all = X_train,y_train,X_val,y_val,X_test,y_test\n",
        "    first=False\n",
        "    continue\n",
        "  X_train_all,y_train_all,X_val_all,y_val_all,X_test_all,y_test_all\\\n",
        "        = np.vstack((X_train_all,X_train)), np.concatenate((y_train_all,y_train)),\\\n",
        "          np.vstack((X_val_all,X_val)), np.concatenate((y_val_all,y_val)),\\\n",
        "          np.vstack((X_test_all,X_test)), np.concatenate((y_test_all,y_test)),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "771bc0b91bfa424798feecbbdf509f6c",
            "ee0f8d07ebc1464b85addc9f7bacb38b",
            "59884878cdfe4609af02f3d53ab41a32",
            "26b2f20063864509980435ba26ca1dfd",
            "335283379e8a4b41a326b6fcc9ce813d",
            "09d359e76459438292279e9b3717e35a",
            "2aa12790fcea47a19de5f7524ef38ad9",
            "b76fe5af46a9439ea0d20fbda26324cd",
            "24ed0717a7994c2dbac90893c56a3d08",
            "99f4348ec77b4c37aa1def60592eaa7f",
            "bd8f780d84784eb9b18bd43fb9fddc6f"
          ]
        },
        "id": "87BJEu-8cRIV",
        "outputId": "b7a58534-2af3-4300-c8ae-3f88ba5fbe72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/55 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "771bc0b91bfa424798feecbbdf509f6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BE\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "ZDGE\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1225, 6)\n",
            "Validation data shape: (153, 6)\n",
            "Test data shape: (153, 6)\n",
            "Training set shape (1097, 128, 5) (1097,)\n",
            "Validation set shape (25, 128, 5) (25,)\n",
            "Testing set shape (25, 128, 5) (25,)\n",
            "KFY\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (4709, 6)\n",
            "Validation data shape: (589, 6)\n",
            "Test data shape: (588, 6)\n",
            "Training set shape (4581, 128, 5) (4581,)\n",
            "Validation set shape (461, 128, 5) (461,)\n",
            "Testing set shape (460, 128, 5) (460,)\n",
            "SUM\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1473, 6)\n",
            "Validation data shape: (184, 6)\n",
            "Test data shape: (184, 6)\n",
            "Training set shape (1345, 128, 5) (1345,)\n",
            "Validation set shape (56, 128, 5) (56,)\n",
            "Testing set shape (56, 128, 5) (56,)\n",
            "CIM\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2939, 6)\n",
            "Validation data shape: (367, 6)\n",
            "Test data shape: (367, 6)\n",
            "Training set shape (2811, 128, 5) (2811,)\n",
            "Validation set shape (239, 128, 5) (239,)\n",
            "Testing set shape (239, 128, 5) (239,)\n",
            "EME\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5536, 6)\n",
            "Validation data shape: (692, 6)\n",
            "Test data shape: (691, 6)\n",
            "Training set shape (5408, 128, 5) (5408,)\n",
            "Validation set shape (564, 128, 5) (564,)\n",
            "Testing set shape (563, 128, 5) (563,)\n",
            "MUI\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (3812, 6)\n",
            "Validation data shape: (477, 6)\n",
            "Test data shape: (476, 6)\n",
            "Training set shape (3684, 128, 5) (3684,)\n",
            "Validation set shape (349, 128, 5) (349,)\n",
            "Testing set shape (348, 128, 5) (348,)\n",
            "RGT\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1759, 6)\n",
            "Validation data shape: (220, 6)\n",
            "Test data shape: (219, 6)\n",
            "Training set shape (1631, 128, 5) (1631,)\n",
            "Validation set shape (92, 128, 5) (92,)\n",
            "Testing set shape (91, 128, 5) (91,)\n",
            "BGX\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2302, 6)\n",
            "Validation data shape: (288, 6)\n",
            "Test data shape: (287, 6)\n",
            "Training set shape (2174, 128, 5) (2174,)\n",
            "Validation set shape (160, 128, 5) (160,)\n",
            "Testing set shape (159, 128, 5) (159,)\n",
            "GOED\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "RES\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (7676, 6)\n",
            "Validation data shape: (959, 6)\n",
            "Test data shape: (959, 6)\n",
            "Training set shape (7548, 128, 5) (7548,)\n",
            "Validation set shape (831, 128, 5) (831,)\n",
            "Testing set shape (831, 128, 5) (831,)\n",
            "CYD\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5548, 6)\n",
            "Validation data shape: (693, 6)\n",
            "Test data shape: (693, 6)\n",
            "Training set shape (5420, 128, 5) (5420,)\n",
            "Validation set shape (565, 128, 5) (565,)\n",
            "Testing set shape (565, 128, 5) (565,)\n",
            "SNMP\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (3147, 6)\n",
            "Validation data shape: (393, 6)\n",
            "Test data shape: (393, 6)\n",
            "Training set shape (3019, 128, 5) (3019,)\n",
            "Validation set shape (265, 128, 5) (265,)\n",
            "Testing set shape (265, 128, 5) (265,)\n",
            "EIX\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (9921, 6)\n",
            "Validation data shape: (1240, 6)\n",
            "Test data shape: (1240, 6)\n",
            "Training set shape (9793, 128, 5) (9793,)\n",
            "Validation set shape (1112, 128, 5) (1112,)\n",
            "Testing set shape (1112, 128, 5) (1112,)\n",
            "SILV\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "BCSF\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "TPZ\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2604, 6)\n",
            "Validation data shape: (326, 6)\n",
            "Test data shape: (325, 6)\n",
            "Training set shape (2476, 128, 5) (2476,)\n",
            "Validation set shape (198, 128, 5) (198,)\n",
            "Testing set shape (197, 128, 5) (197,)\n",
            "TDC\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2972, 6)\n",
            "Validation data shape: (372, 6)\n",
            "Test data shape: (371, 6)\n",
            "Training set shape (2844, 128, 5) (2844,)\n",
            "Validation set shape (244, 128, 5) (244,)\n",
            "Testing set shape (243, 128, 5) (243,)\n",
            "NBB\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2454, 6)\n",
            "Validation data shape: (307, 6)\n",
            "Test data shape: (306, 6)\n",
            "Training set shape (2326, 128, 5) (2326,)\n",
            "Validation set shape (179, 128, 5) (179,)\n",
            "Testing set shape (178, 128, 5) (178,)\n",
            "JMP\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "\n",
            "1 Failed download:\n",
            "- JMP: No data found, symbol may be delisted\n",
            "Cannot featch the data! Ticker not found.\n",
            "NGL\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2242, 6)\n",
            "Validation data shape: (280, 6)\n",
            "Test data shape: (280, 6)\n",
            "Training set shape (2114, 128, 5) (2114,)\n",
            "Validation set shape (152, 128, 5) (152,)\n",
            "Testing set shape (152, 128, 5) (152,)\n",
            "FHN\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8532, 6)\n",
            "Validation data shape: (1067, 6)\n",
            "Test data shape: (1066, 6)\n",
            "Training set shape (8404, 128, 5) (8404,)\n",
            "Validation set shape (939, 128, 5) (939,)\n",
            "Testing set shape (938, 128, 5) (938,)\n",
            "FEI\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1932, 6)\n",
            "Validation data shape: (242, 6)\n",
            "Test data shape: (241, 6)\n",
            "Training set shape (1804, 128, 5) (1804,)\n",
            "Validation set shape (114, 128, 5) (114,)\n",
            "Testing set shape (113, 128, 5) (113,)\n",
            "ZTO\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1144, 6)\n",
            "Validation data shape: (143, 6)\n",
            "Test data shape: (142, 6)\n",
            "Training set shape (1016, 128, 5) (1016,)\n",
            "Validation set shape (15, 128, 5) (15,)\n",
            "Testing set shape (14, 128, 5) (14,)\n",
            "MSD\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5836, 6)\n",
            "Validation data shape: (729, 6)\n",
            "Test data shape: (729, 6)\n",
            "Training set shape (5708, 128, 5) (5708,)\n",
            "Validation set shape (601, 128, 5) (601,)\n",
            "Testing set shape (601, 128, 5) (601,)\n",
            "JHS\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8532, 6)\n",
            "Validation data shape: (1067, 6)\n",
            "Test data shape: (1066, 6)\n",
            "Training set shape (8404, 128, 5) (8404,)\n",
            "Validation set shape (939, 128, 5) (939,)\n",
            "Testing set shape (938, 128, 5) (938,)\n",
            "XIN\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2932, 6)\n",
            "Validation data shape: (366, 6)\n",
            "Test data shape: (366, 6)\n",
            "Training set shape (2804, 128, 5) (2804,)\n",
            "Validation set shape (238, 128, 5) (238,)\n",
            "Testing set shape (238, 128, 5) (238,)\n",
            "WMT\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (10057, 6)\n",
            "Validation data shape: (1257, 6)\n",
            "Test data shape: (1257, 6)\n",
            "Training set shape (9929, 128, 5) (9929,)\n",
            "Validation set shape (1129, 128, 5) (1129,)\n",
            "Testing set shape (1129, 128, 5) (1129,)\n",
            "YPF\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5845, 6)\n",
            "Validation data shape: (731, 6)\n",
            "Test data shape: (730, 6)\n",
            "Training set shape (5717, 128, 5) (5717,)\n",
            "Validation set shape (603, 128, 5) (603,)\n",
            "Testing set shape (602, 128, 5) (602,)\n",
            "RVLV\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "PCPL\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "\n",
            "1 Failed download:\n",
            "- PCPL: No data found, symbol may be delisted\n",
            "Cannot featch the data! Ticker not found.\n",
            "GTN\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (3996, 6)\n",
            "Validation data shape: (499, 6)\n",
            "Test data shape: (499, 6)\n",
            "Training set shape (3868, 128, 5) (3868,)\n",
            "Validation set shape (371, 128, 5) (371,)\n",
            "Testing set shape (371, 128, 5) (371,)\n",
            "POST\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2100, 6)\n",
            "Validation data shape: (263, 6)\n",
            "Test data shape: (262, 6)\n",
            "Training set shape (1972, 128, 5) (1972,)\n",
            "Validation set shape (135, 128, 5) (135,)\n",
            "Testing set shape (134, 128, 5) (134,)\n",
            "LH\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (6503, 6)\n",
            "Validation data shape: (813, 6)\n",
            "Test data shape: (812, 6)\n",
            "Training set shape (6375, 128, 5) (6375,)\n",
            "Validation set shape (685, 128, 5) (685,)\n",
            "Testing set shape (684, 128, 5) (684,)\n",
            "SXI\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (9961, 6)\n",
            "Validation data shape: (1245, 6)\n",
            "Test data shape: (1245, 6)\n",
            "Training set shape (9833, 128, 5) (9833,)\n",
            "Validation set shape (1117, 128, 5) (1117,)\n",
            "Testing set shape (1117, 128, 5) (1117,)\n",
            "AZEK\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "EPM\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5282, 6)\n",
            "Validation data shape: (660, 6)\n",
            "Test data shape: (660, 6)\n",
            "Training set shape (5154, 128, 5) (5154,)\n",
            "Validation set shape (532, 128, 5) (532,)\n",
            "Testing set shape (532, 128, 5) (532,)\n",
            "GOF\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (3008, 6)\n",
            "Validation data shape: (376, 6)\n",
            "Test data shape: (376, 6)\n",
            "Training set shape (2880, 128, 5) (2880,)\n",
            "Validation set shape (248, 128, 5) (248,)\n",
            "Testing set shape (248, 128, 5) (248,)\n",
            "HDB\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (4217, 6)\n",
            "Validation data shape: (527, 6)\n",
            "Test data shape: (527, 6)\n",
            "Training set shape (4089, 128, 5) (4089,)\n",
            "Validation set shape (399, 128, 5) (399,)\n",
            "Testing set shape (399, 128, 5) (399,)\n",
            "PCM\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (5812, 6)\n",
            "Validation data shape: (726, 6)\n",
            "Test data shape: (726, 6)\n",
            "Training set shape (5684, 128, 5) (5684,)\n",
            "Validation set shape (598, 128, 5) (598,)\n",
            "Testing set shape (598, 128, 5) (598,)\n",
            "BXMT\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8374, 6)\n",
            "Validation data shape: (1047, 6)\n",
            "Test data shape: (1046, 6)\n",
            "Training set shape (8246, 128, 5) (8246,)\n",
            "Validation set shape (919, 128, 5) (919,)\n",
            "Testing set shape (918, 128, 5) (918,)\n",
            "ACU\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8532, 6)\n",
            "Validation data shape: (1067, 6)\n",
            "Test data shape: (1066, 6)\n",
            "Training set shape (8404, 128, 5) (8404,)\n",
            "Validation set shape (939, 128, 5) (939,)\n",
            "Testing set shape (938, 128, 5) (938,)\n",
            "SHLX\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1546, 6)\n",
            "Validation data shape: (193, 6)\n",
            "Test data shape: (193, 6)\n",
            "Training set shape (1418, 128, 5) (1418,)\n",
            "Validation set shape (65, 128, 5) (65,)\n",
            "Testing set shape (65, 128, 5) (65,)\n",
            "DNB\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Stock Data Not Big Enough!\n",
            "MTR\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8617, 6)\n",
            "Validation data shape: (1077, 6)\n",
            "Test data shape: (1077, 6)\n",
            "Training set shape (8489, 128, 5) (8489,)\n",
            "Validation set shape (949, 128, 5) (949,)\n",
            "Testing set shape (949, 128, 5) (949,)\n",
            "DTQ\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "\n",
            "1 Failed download:\n",
            "- DTQ: No data found, symbol may be delisted\n",
            "Cannot featch the data! Ticker not found.\n",
            "ARGD\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (1967, 6)\n",
            "Validation data shape: (246, 6)\n",
            "Test data shape: (245, 6)\n",
            "Training set shape (1839, 128, 5) (1839,)\n",
            "Validation set shape (118, 128, 5) (118,)\n",
            "Testing set shape (117, 128, 5) (117,)\n",
            "ASH\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8532, 6)\n",
            "Validation data shape: (1067, 6)\n",
            "Test data shape: (1066, 6)\n",
            "Training set shape (8404, 128, 5) (8404,)\n",
            "Validation set shape (939, 128, 5) (939,)\n",
            "Testing set shape (938, 128, 5) (938,)\n",
            "BGIO\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "\n",
            "1 Failed download:\n",
            "- BGIO: No data found, symbol may be delisted\n",
            "Cannot featch the data! Ticker not found.\n",
            "CVE\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2542, 6)\n",
            "Validation data shape: (318, 6)\n",
            "Test data shape: (317, 6)\n",
            "Training set shape (2414, 128, 5) (2414,)\n",
            "Validation set shape (190, 128, 5) (190,)\n",
            "Testing set shape (189, 128, 5) (189,)\n",
            "AAPL\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (8382, 6)\n",
            "Validation data shape: (1048, 6)\n",
            "Test data shape: (1047, 6)\n",
            "Training set shape (8254, 128, 5) (8254,)\n",
            "Validation set shape (920, 128, 5) (920,)\n",
            "Testing set shape (919, 128, 5) (919,)\n",
            "IBM\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (12184, 6)\n",
            "Validation data shape: (1523, 6)\n",
            "Test data shape: (1523, 6)\n",
            "Training set shape (12056, 128, 5) (12056,)\n",
            "Validation set shape (1395, 128, 5) (1395,)\n",
            "Testing set shape (1395, 128, 5) (1395,)\n",
            "QQQ\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (4695, 6)\n",
            "Validation data shape: (587, 6)\n",
            "Test data shape: (586, 6)\n",
            "Training set shape (4567, 128, 5) (4567,)\n",
            "Validation set shape (459, 128, 5) (459,)\n",
            "Testing set shape (458, 128, 5) (458,)\n",
            "NVDA\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (4720, 6)\n",
            "Validation data shape: (590, 6)\n",
            "Test data shape: (589, 6)\n",
            "Training set shape (4592, 128, 5) (4592,)\n",
            "Validation set shape (462, 128, 5) (462,)\n",
            "Testing set shape (461, 128, 5) (461,)\n",
            "TSLA\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (2420, 6)\n",
            "Validation data shape: (302, 6)\n",
            "Test data shape: (302, 6)\n",
            "Training set shape (2292, 128, 5) (2292,)\n",
            "Validation set shape (174, 128, 5) (174,)\n",
            "Testing set shape (174, 128, 5) (174,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKX_QDCtchHe",
        "outputId": "a0da5724-0d19-42c1-9b74-f9f8269ef9c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ZDGE', (0.6600000262260437, 16.940000534057617), (1225, 153, 153)),\n",
              " ('KFY', (5.253301620483398, 44.125), (4709, 589, 588)),\n",
              " ('SUM', (7.510000228881836, 34.060001373291016), (1473, 184, 184)),\n",
              " ('CIM', (1.5924321413040161, 98.94999694824219), (2939, 367, 367)),\n",
              " ('EME', (0.9336391091346741, 73.44000244140625), (5536, 692, 691)),\n",
              " ('MUI', (3.992082118988037, 17.8799991607666), (3812, 477, 476)),\n",
              " ('RGT', (4.639738082885742, 13.220000267028809), (1759, 220, 219)),\n",
              " ('BGX', (6.893987655639648, 21.5), (2302, 288, 287)),\n",
              " ('RES', (0.0, 25.149999618530273), (7676, 959, 959)),\n",
              " ('CYD', (0.08931300789117813, 37.2400016784668), (5548, 693, 693)),\n",
              " ('SNMP', (1.2317560911178589, 507.3999938964844), (3147, 393, 393)),\n",
              " ('EIX', (0.0, 60.2599983215332), (9921, 1240, 1240)),\n",
              " ('TPZ', (7.088333606719971, 29.719999313354492), (2604, 326, 325)),\n",
              " ('TDC', (11.109999656677246, 80.97000122070312), (2972, 372, 371)),\n",
              " ('NBB', (8.553238868713379, 23.299999237060547), (2454, 307, 306)),\n",
              " ('NGL', (1.149999976158142, 46.25), (2242, 280, 280)),\n",
              " ('FHN', (0.0, 46.46669006347656), (8532, 1067, 1066)),\n",
              " ('FEI', (2.328829765319824, 22.34000015258789), (1932, 242, 241)),\n",
              " ('ZTO', (10.619209289550781, 38.9900016784668), (1144, 143, 142)),\n",
              " ('MSD', (0.7716057896614075, 18.375), (5836, 729, 729)),\n",
              " ('JHS', (0.0, 18.375), (8532, 1067, 1066)),\n",
              " ('XIN', (0.8297678232192993, 17.979999542236328), (2932, 366, 366)),\n",
              " ('WMT', (0.00882316380739212, 71.36000061035156), (10057, 1257, 1257)),\n",
              " ('YPF', (3.2030582427978516, 69.9800033569336), (5845, 731, 730)),\n",
              " ('GTN', (0.1599999964237213, 18.06999969482422), (3996, 499, 499)),\n",
              " ('POST', (16.583770751953125, 74.43062591552734), (2100, 263, 262)),\n",
              " ('LH', (2.8043265342712402, 131.19000244140625), (6503, 813, 812)),\n",
              " ('SXI', (0.0, 46.83000183105469), (9961, 1245, 1245)),\n",
              " ('EPM', (0.1337597370147705, 19000.0), (5282, 660, 660)),\n",
              " ('GOF', (1.7750091552734375, 24.139999389648438), (3008, 376, 376)),\n",
              " ('HDB', (1.0613754987716675, 55.3849983215332), (4217, 527, 527)),\n",
              " ('PCM', (0.7234127521514893, 15.5), (5812, 726, 726)),\n",
              " ('BXMT', (0.0, 559.7000122070312), (8374, 1047, 1046)),\n",
              " ('ACU', (0.0, 21.0), (8532, 1067, 1066)),\n",
              " ('SHLX', (5.6163434982299805, 49.77000045776367), (1546, 193, 193)),\n",
              " ('MTR', (0.0, 86.86000061035156), (8617, 1077, 1077)),\n",
              " ('ARGD', (9.600000381469727, 27.43000030517578), (1967, 246, 245)),\n",
              " ('ASH', (0.0, 49.13405227661133), (8532, 1067, 1066)),\n",
              " ('CVE', (0.15172535181045532, 40.72999954223633), (2542, 318, 317)),\n",
              " ('AAPL', (0.0383291020989418, 25.18107032775879), (8382, 1048, 1047)),\n",
              " ('IBM', (0.8954331278800964, 133.06643676757812), (12184, 1523, 1523)),\n",
              " ('QQQ', (17.321014404296875, 152.8300018310547), (4695, 587, 586)),\n",
              " ('NVDA', (0.31326064467430115, 49.897499084472656), (4720, 590, 589)),\n",
              " ('TSLA', (2.996000051498413, 193.79800415039062), (2420, 302, 302))]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all.shape,y_train_all.shape,X_val_all.shape,y_val_all.shape,X_test_all.shape,y_test_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm3OQO8ecj3x",
        "outputId": "26be4371-f4f3-4f01-d409-641734712d2b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((212887, 128, 5),\n",
              " (212887,),\n",
              " (21684, 128, 5),\n",
              " (21684,),\n",
              " (21661, 128, 5),\n",
              " (21661,))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_train_all).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmwtXvCvlOyY",
        "outputId": "8039da73-04b2-4820-b51f-2998f99555ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    120787\n",
              "0.0     92100\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZKxcbnfnfZJ",
        "outputId": "634763e3-a157-4b16-f561-f760eae74d52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(212887, 128, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Time2Vector(Layer):\n",
        "  def __init__(self, seq_len, **kwargs):\n",
        "    super(Time2Vector, self).__init__()\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    '''Initialize weights and biases with shape (batch, seq_len)'''\n",
        "    self.weights_linear = self.add_weight(name='weight_linear',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "    \n",
        "    self.bias_linear = self.add_weight(name='bias_linear',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "    \n",
        "    self.weights_periodic = self.add_weight(name='weight_periodic',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "    self.bias_periodic = self.add_weight(name='bias_periodic',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    '''Calculate linear and periodic time features'''\n",
        "    x = tf.math.reduce_mean(x[:,:,:4], axis=-1) \n",
        "    time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
        "    time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
        "    \n",
        "    time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
        "    time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
        "    return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
        "   \n",
        "  def get_config(self): # Needed for saving and loading model with custom layer\n",
        "    config = super().get_config().copy()\n",
        "    config.update({'seq_len': self.seq_len})\n",
        "    return config"
      ],
      "metadata": {
        "id": "7re6APIlcnrc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleAttention(Layer):\n",
        "  def __init__(self, d_k, d_v):\n",
        "    super(SingleAttention, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.query = Dense(self.d_k, \n",
        "                       input_shape=input_shape, \n",
        "                       kernel_initializer='glorot_uniform', \n",
        "                       bias_initializer='glorot_uniform')\n",
        "    \n",
        "    self.key = Dense(self.d_k, \n",
        "                     input_shape=input_shape, \n",
        "                     kernel_initializer='glorot_uniform', \n",
        "                     bias_initializer='glorot_uniform')\n",
        "    \n",
        "    self.value = Dense(self.d_v, \n",
        "                       input_shape=input_shape, \n",
        "                       kernel_initializer='glorot_uniform', \n",
        "                       bias_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
        "    q = self.query(inputs[0])\n",
        "    k = self.key(inputs[1])\n",
        "\n",
        "    attn_weights = tf.matmul(q, k, transpose_b=True)\n",
        "    attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
        "    attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
        "    \n",
        "    v = self.value(inputs[2])\n",
        "    attn_out = tf.matmul(attn_weights, v)\n",
        "    return attn_out    \n",
        "\n",
        "#############################################################################\n",
        "\n",
        "class MultiAttention(Layer):\n",
        "  def __init__(self, d_k, d_v, n_heads):\n",
        "    super(MultiAttention, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.n_heads = n_heads\n",
        "    self.attn_heads = list()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    for n in range(self.n_heads):\n",
        "      self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
        "    \n",
        "    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n",
        "    self.linear = Dense(input_shape[0][-1], \n",
        "                        input_shape=input_shape, \n",
        "                        kernel_initializer='glorot_uniform', \n",
        "                        bias_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
        "    concat_attn = tf.concat(attn, axis=-1)\n",
        "    multi_linear = self.linear(concat_attn)\n",
        "    return multi_linear   \n",
        "\n",
        "#############################################################################\n",
        "\n",
        "class TransformerEncoder(Layer):\n",
        "  def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.n_heads = n_heads\n",
        "    self.ff_dim = ff_dim\n",
        "    self.attn_heads = list()\n",
        "    self.dropout_rate = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
        "    self.attn_dropout = Dropout(self.dropout_rate)\n",
        "    self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
        "\n",
        "    self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
        "    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 \n",
        "    self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n",
        "    self.ff_dropout = Dropout(self.dropout_rate)\n",
        "    self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n",
        "  \n",
        "  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
        "    attn_layer = self.attn_multi(inputs)\n",
        "    attn_layer = self.attn_dropout(attn_layer)\n",
        "    attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
        "\n",
        "    ff_layer = self.ff_conv1D_1(attn_layer)\n",
        "    ff_layer = self.ff_conv1D_2(ff_layer)\n",
        "    ff_layer = self.ff_dropout(ff_layer)\n",
        "    ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
        "    return ff_layer \n",
        "\n",
        "  def get_config(self): # Needed for saving and loading model with custom layer\n",
        "    config = super().get_config().copy()\n",
        "    config.update({'d_k': self.d_k,\n",
        "                   'd_v': self.d_v,\n",
        "                   'n_heads': self.n_heads,\n",
        "                   'ff_dim': self.ff_dim,\n",
        "                   'attn_heads': self.attn_heads,\n",
        "                   'dropout_rate': self.dropout_rate})\n",
        "    return config          "
      ],
      "metadata": {
        "id": "7urkpDI_ctUG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  '''Initialize time and transformer layers'''\n",
        "  time_embedding = Time2Vector(seq_len)\n",
        "  attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "\n",
        "  '''Construct model'''\n",
        "  in_seq = Input(shape=(seq_len, 5))\n",
        "  x = time_embedding(in_seq)\n",
        "  x = Concatenate(axis=-1)([in_seq, x])\n",
        "  x = attn_layer1((x, x, x))\n",
        "  x = attn_layer2((x, x, x))\n",
        "  x = attn_layer3((x, x, x))\n",
        "  x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  x = Dense(64, activation='relu')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs=in_seq, outputs=out)\n",
        "  model.compile(loss='mse', optimizer='adam', metrics=['mae', 'accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "k3X3H76yc4Wz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=.80)\n",
        "sess=tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
        "model = create_model()\n",
        "model.summary()\n",
        "\n",
        "NAME=f\"StockTranformers{datetime.datetime.now().strftime('%m%d%y')}\"\n",
        "\n",
        "log_dir = f'./logs/{NAME}'\n",
        "tensorboard=tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "callback = tf.keras.callbacks.ModelCheckpoint('Transformer+TimeEmbedding.hdf5', \n",
        "                                              monitor='val_loss', \n",
        "                                              save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YeCC8iRc_il",
        "outputId": "30ce9fb7-697e-49d9-f206-8818013578f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 5)]     0           []                               \n",
            "                                                                                                  \n",
            " time2_vector (Time2Vector)     (None, 128, 2)       512         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 128, 7)       0           ['input_1[0][0]',                \n",
            "                                                                  'time2_vector[0][0]']           \n",
            "                                                                                                  \n",
            " transformer_encoder (Transform  (None, 128, 7)      99114       ['concatenate[0][0]',            \n",
            " erEncoder)                                                       'concatenate[0][0]',            \n",
            "                                                                  'concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Transfo  (None, 128, 7)      99114       ['transformer_encoder[0][0]',    \n",
            " rmerEncoder)                                                     'transformer_encoder[0][0]',    \n",
            "                                                                  'transformer_encoder[0][0]']    \n",
            "                                                                                                  \n",
            " transformer_encoder_2 (Transfo  (None, 128, 7)      99114       ['transformer_encoder_1[0][0]',  \n",
            " rmerEncoder)                                                     'transformer_encoder_1[0][0]',  \n",
            "                                                                  'transformer_encoder_1[0][0]']  \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 128)         0           ['transformer_encoder_2[0][0]']  \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 128)          0           ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 64)           8256        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 64)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            65          ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 306,175\n",
            "Trainable params: 306,175\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "import numpy as np   \n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return batch_x, batch_y\n",
        "\n",
        "train_gen = DataGenerator(X_train_all, y_train_all, 32)\n",
        "val_gen = DataGenerator(X_val_all, y_val_all, 32)"
      ],
      "metadata": {
        "id": "rUx2QvTTcv6s"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not LOAD_MODEL:\n",
        "  history = model.fit(train_gen,  \n",
        "                      epochs=2, \n",
        "                      callbacks=[callback,tensorboard],\n",
        "                      validation_data=val_gen)  \n",
        "\n",
        "  model.save('stock_pred_transformers')"
      ],
      "metadata": {
        "id": "Ctw4WjLHdXLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ada190a-5524-4790-be9c-cdbcc41a531c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "6653/6653 [==============================] - ETA: 0s - loss: 0.2457 - mae: 0.4912 - accuracy: 0.5673\n",
            "Epoch 1: val_loss improved from inf to 0.25033, saving model to Transformer+TimeEmbedding.hdf5\n",
            "6653/6653 [==============================] - 16695s 3s/step - loss: 0.2457 - mae: 0.4912 - accuracy: 0.5673 - val_loss: 0.2503 - val_mae: 0.4959 - val_accuracy: 0.5309\n",
            "Epoch 2/2\n",
            "6653/6653 [==============================] - ETA: 0s - loss: 0.2456 - mae: 0.4910 - accuracy: 0.5674\n",
            "Epoch 2: val_loss did not improve from 0.25033\n",
            "6653/6653 [==============================] - 16546s 2s/step - loss: 0.2456 - mae: 0.4910 - accuracy: 0.5674 - val_loss: 0.2504 - val_mae: 0.4958 - val_accuracy: 0.5309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_attention_layer_call_fn, multi_attention_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn while saving (showing 5 of 336). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: stock_pred_transformers/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: stock_pred_transformers/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('Transformer+TimeEmbedding.hdf5',\n",
        "                                   custom_objects={'Time2Vector': Time2Vector, \n",
        "                                                   'SingleAttention': SingleAttention,\n",
        "                                                   'MultiAttention': MultiAttention,\n",
        "                                                   'TransformerEncoder': TransformerEncoder})"
      ],
      "metadata": {
        "id": "NXhYjabHdcnE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate predication for training, validation and test data\n",
        "# train_pred = model.predict(X_train_all)\n",
        "# val_pred = model.predict(X_val_all)\n",
        "# test_pred = model.predict(X_test_all)\n",
        "\n",
        "# #Print evaluation metrics for all datasets\n",
        "# train_eval = model.evaluate(X_train_all, y_train_all, verbose=0)\n",
        "# val_eval = model.evaluate(X_val_all, y_val_all, verbose=0)\n",
        "# test_eval = model.evaluate(X_test_all, y_test_all, verbose=0)\n",
        "\n",
        "# print(' ')\n",
        "# print('Evaluation metrics')\n",
        "# print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval[0], train_eval[1], train_eval[2]))\n",
        "# print('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval[0], val_eval[1], val_eval[2]))\n",
        "# print('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval[0], test_eval[1], test_eval[2]))"
      ],
      "metadata": {
        "id": "xSzJ--3TdjHm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_data(ticker='IBM'):\n",
        "   df_ibm = yf.download(ticker).reset_index()\n",
        "   df_ibm = df_ibm.drop(columns='Close')\n",
        "   df_ibm.columns=['Date','Open','High','Low','Close','Volume']\n",
        "   train_data,val_data,test_data,min_max = preprocessing(df_ibm)\n",
        "   X_train,y_train,X_val,y_val,X_test,y_test = train_test_split(train_data,val_data,test_data)\n",
        "   return (X_test,y_test,min_max)"
      ],
      "metadata": {
        "id": "wrOzFO5odtgd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_X_test,my_y_test,my_min_max = get_test_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acUxPzaeeRmt",
        "outputId": "1356fa4f-d944-40e4-fb28-3788d8f33a2e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "Training data shape: (12184, 6)\n",
            "Validation data shape: (1523, 6)\n",
            "Test data shape: (1523, 6)\n",
            "Training set shape (12056, 128, 5) (12056,)\n",
            "Validation set shape (1395, 128, 5) (1395,)\n",
            "Testing set shape (1395, 128, 5) (1395,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_pred = model.predict(my_X_test)"
      ],
      "metadata": {
        "id": "vEmvw60PeitO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(my_pred<=0.5).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSbwFStYPhfj",
        "outputId": "686d6e73-3851-469d-e5c1-d697dfc1fe7a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_pred.flatten().tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BER6LtbvOcS9",
        "outputId": "fd48fb02-e7de-463b-fdc4-1dc37751c359"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5668294429779053,\n",
              " 0.5668293237686157,\n",
              " 0.566829264163971,\n",
              " 0.5668293237686157,\n",
              " 0.5668293833732605,\n",
              " 0.566829264163971,\n",
              " 0.5668294429779053,\n",
              " 0.5668293237686157,\n",
              " 0.5668293833732605,\n",
              " 0.56682950258255,\n",
              " 0.56682950258255,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668299794197083,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.566830039024353,\n",
              " 0.566830039024353,\n",
              " 0.5668301582336426,\n",
              " 0.5668301582336426,\n",
              " 0.5668302774429321,\n",
              " 0.5668302178382874,\n",
              " 0.5668299794197083,\n",
              " 0.5668301582336426,\n",
              " 0.5668300986289978,\n",
              " 0.5668301582336426,\n",
              " 0.5668300986289978,\n",
              " 0.5668302178382874,\n",
              " 0.5668301582336426,\n",
              " 0.5668303370475769,\n",
              " 0.5668302178382874,\n",
              " 0.5668303370475769,\n",
              " 0.5668303370475769,\n",
              " 0.5668302774429321,\n",
              " 0.5668303370475769,\n",
              " 0.5668302178382874,\n",
              " 0.5668301582336426,\n",
              " 0.5668302774429321,\n",
              " 0.5668301582336426,\n",
              " 0.5668302178382874,\n",
              " 0.5668300986289978,\n",
              " 0.5668302178382874,\n",
              " 0.5668302774429321,\n",
              " 0.5668302178382874,\n",
              " 0.5668303370475769,\n",
              " 0.5668303370475769,\n",
              " 0.5668302774429321,\n",
              " 0.5668303370475769,\n",
              " 0.5668303370475769,\n",
              " 0.5668301582336426,\n",
              " 0.5668303966522217,\n",
              " 0.5668302774429321,\n",
              " 0.5668302774429321,\n",
              " 0.5668302774429321,\n",
              " 0.5668303966522217,\n",
              " 0.5668305158615112,\n",
              " 0.5668302774429321,\n",
              " 0.5668302178382874,\n",
              " 0.5668301582336426,\n",
              " 0.5668301582336426,\n",
              " 0.5668303370475769,\n",
              " 0.5668302774429321,\n",
              " 0.5668303370475769,\n",
              " 0.5668303370475769,\n",
              " 0.5668301582336426,\n",
              " 0.5668301582336426,\n",
              " 0.5668301582336426,\n",
              " 0.5668302178382874,\n",
              " 0.5668302774429321,\n",
              " 0.566830039024353,\n",
              " 0.5668300986289978,\n",
              " 0.5668298602104187,\n",
              " 0.5668298006057739,\n",
              " 0.5668299198150635,\n",
              " 0.5668298602104187,\n",
              " 0.5668298006057739,\n",
              " 0.5668299794197083,\n",
              " 0.5668299198150635,\n",
              " 0.566830039024353,\n",
              " 0.5668299794197083,\n",
              " 0.5668300986289978,\n",
              " 0.5668301582336426,\n",
              " 0.5668299794197083,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.5668297410011292,\n",
              " 0.5668296217918396,\n",
              " 0.5668295621871948,\n",
              " 0.56682950258255,\n",
              " 0.56682950258255,\n",
              " 0.5668296217918396,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.5668296217918396,\n",
              " 0.5668296813964844,\n",
              " 0.56682950258255,\n",
              " 0.5668296217918396,\n",
              " 0.5668294429779053,\n",
              " 0.5668294429779053,\n",
              " 0.5668294429779053,\n",
              " 0.5668293833732605,\n",
              " 0.5668294429779053,\n",
              " 0.5668293833732605,\n",
              " 0.5668293237686157,\n",
              " 0.5668293237686157,\n",
              " 0.5668293237686157,\n",
              " 0.566829264163971,\n",
              " 0.5668292045593262,\n",
              " 0.5668293237686157,\n",
              " 0.5668291449546814,\n",
              " 0.5668291449546814,\n",
              " 0.5668289661407471,\n",
              " 0.5668290257453918,\n",
              " 0.5668288469314575,\n",
              " 0.5668289065361023,\n",
              " 0.5668289065361023,\n",
              " 0.5668287873268127,\n",
              " 0.5668287873268127,\n",
              " 0.5668287873268127,\n",
              " 0.566828727722168,\n",
              " 0.5668287873268127,\n",
              " 0.5668286681175232,\n",
              " 0.5668286085128784,\n",
              " 0.5668285489082336,\n",
              " 0.5668285489082336,\n",
              " 0.5668284893035889,\n",
              " 0.5668287873268127,\n",
              " 0.5668286085128784,\n",
              " 0.5668286681175232,\n",
              " 0.5668285489082336,\n",
              " 0.5668283700942993,\n",
              " 0.5668286085128784,\n",
              " 0.5668284296989441,\n",
              " 0.5668286681175232,\n",
              " 0.5668286085128784,\n",
              " 0.5668285489082336,\n",
              " 0.5668286085128784,\n",
              " 0.5668284893035889,\n",
              " 0.5668285489082336,\n",
              " 0.5668288469314575,\n",
              " 0.5668288469314575,\n",
              " 0.5668289661407471,\n",
              " 0.5668290257453918,\n",
              " 0.5668289661407471,\n",
              " 0.5668290853500366,\n",
              " 0.5668290257453918,\n",
              " 0.5668288469314575,\n",
              " 0.5668290853500366,\n",
              " 0.5668290853500366,\n",
              " 0.5668291449546814,\n",
              " 0.5668290853500366,\n",
              " 0.5668291449546814,\n",
              " 0.5668292045593262,\n",
              " 0.5668292045593262,\n",
              " 0.566829264163971,\n",
              " 0.5668293833732605,\n",
              " 0.5668294429779053,\n",
              " 0.56682950258255,\n",
              " 0.56682950258255,\n",
              " 0.5668293833732605,\n",
              " 0.5668294429779053,\n",
              " 0.56682950258255,\n",
              " 0.56682950258255,\n",
              " 0.5668295621871948,\n",
              " 0.5668294429779053,\n",
              " 0.5668293833732605,\n",
              " 0.5668296813964844,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668297410011292,\n",
              " 0.5668298006057739,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668296217918396,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.5668298006057739,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668299198150635,\n",
              " 0.5668298602104187,\n",
              " 0.5668295621871948,\n",
              " 0.5668298006057739,\n",
              " 0.5668297410011292,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668293833732605,\n",
              " 0.5668293237686157,\n",
              " 0.5668293833732605,\n",
              " 0.5668293833732605,\n",
              " 0.5668295621871948,\n",
              " 0.5668293237686157,\n",
              " 0.5668293237686157,\n",
              " 0.566829264163971,\n",
              " 0.5668293237686157,\n",
              " 0.56682950258255,\n",
              " 0.5668296813964844,\n",
              " 0.5668296217918396,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668293237686157,\n",
              " 0.566829264163971,\n",
              " 0.5668294429779053,\n",
              " 0.566829264163971,\n",
              " 0.5668293237686157,\n",
              " 0.56682950258255,\n",
              " 0.5668294429779053,\n",
              " 0.5668294429779053,\n",
              " 0.56682950258255,\n",
              " 0.566829264163971,\n",
              " 0.5668293833732605,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668296217918396,\n",
              " 0.5668294429779053,\n",
              " 0.5668296217918396,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668298602104187,\n",
              " 0.5668298006057739,\n",
              " 0.5668299198150635,\n",
              " 0.5668297410011292,\n",
              " 0.5668297410011292,\n",
              " 0.5668299198150635,\n",
              " 0.5668298602104187,\n",
              " 0.566830039024353,\n",
              " 0.566830039024353,\n",
              " 0.5668301582336426,\n",
              " 0.5668299794197083,\n",
              " 0.5668300986289978,\n",
              " 0.5668302178382874,\n",
              " 0.5668303370475769,\n",
              " 0.5668302774429321,\n",
              " 0.5668302774429321,\n",
              " 0.566830039024353,\n",
              " 0.566830039024353,\n",
              " 0.5668302178382874,\n",
              " 0.5668302774429321,\n",
              " 0.5668304562568665,\n",
              " 0.5668302774429321,\n",
              " 0.5668301582336426,\n",
              " 0.5668302178382874,\n",
              " 0.566830039024353,\n",
              " 0.5668302178382874,\n",
              " 0.5668301582336426,\n",
              " 0.5668301582336426,\n",
              " 0.566830039024353,\n",
              " 0.566830039024353,\n",
              " 0.5668299794197083,\n",
              " 0.5668299198150635,\n",
              " 0.566830039024353,\n",
              " 0.566830039024353,\n",
              " 0.566830039024353,\n",
              " 0.5668299198150635,\n",
              " 0.5668298006057739,\n",
              " 0.5668298602104187,\n",
              " 0.5668299198150635,\n",
              " 0.5668299794197083,\n",
              " 0.566830039024353,\n",
              " 0.5668300986289978,\n",
              " 0.566830039024353,\n",
              " 0.5668301582336426,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.5668298602104187,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668296217918396,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.5668299794197083,\n",
              " 0.5668298602104187,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668299198150635,\n",
              " 0.566830039024353,\n",
              " 0.5668300986289978,\n",
              " 0.5668299198150635,\n",
              " 0.5668298006057739,\n",
              " 0.5668299794197083,\n",
              " 0.5668296217918396,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668299794197083,\n",
              " 0.5668299794197083,\n",
              " 0.5668298006057739,\n",
              " 0.5668297410011292,\n",
              " 0.56682950258255,\n",
              " 0.5668294429779053,\n",
              " 0.5668299794197083,\n",
              " 0.5668298602104187,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668297410011292,\n",
              " 0.566830039024353,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668296813964844,\n",
              " 0.5668293833732605,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.5668296217918396,\n",
              " 0.5668295621871948,\n",
              " 0.56682950258255,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668296217918396,\n",
              " 0.5668298006057739,\n",
              " 0.56682950258255,\n",
              " 0.56682950258255,\n",
              " 0.5668293833732605,\n",
              " 0.5668294429779053,\n",
              " 0.5668293833732605,\n",
              " 0.5668293237686157,\n",
              " 0.5668296217918396,\n",
              " 0.5668294429779053,\n",
              " 0.5668296217918396,\n",
              " 0.5668294429779053,\n",
              " 0.5668294429779053,\n",
              " 0.56682950258255,\n",
              " 0.5668296813964844,\n",
              " 0.5668294429779053,\n",
              " 0.5668296217918396,\n",
              " 0.5668294429779053,\n",
              " 0.5668290853500366,\n",
              " 0.5668291449546814,\n",
              " 0.5668292045593262,\n",
              " 0.5668292045593262,\n",
              " 0.5668292045593262,\n",
              " 0.5668292045593262,\n",
              " 0.566829264163971,\n",
              " 0.5668291449546814,\n",
              " 0.5668294429779053,\n",
              " 0.5668294429779053,\n",
              " 0.5668293833732605,\n",
              " 0.5668291449546814,\n",
              " 0.566829264163971,\n",
              " 0.5668290853500366,\n",
              " 0.5668291449546814,\n",
              " 0.5668290853500366,\n",
              " 0.5668289065361023,\n",
              " 0.5668290257453918,\n",
              " 0.5668289065361023,\n",
              " 0.5668292045593262,\n",
              " 0.5668292045593262,\n",
              " 0.5668292045593262,\n",
              " 0.5668292045593262,\n",
              " 0.566829264163971,\n",
              " 0.5668293237686157,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668293833732605,\n",
              " 0.56682950258255,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.5668294429779053,\n",
              " 0.5668293237686157,\n",
              " 0.566829264163971,\n",
              " 0.5668293833732605,\n",
              " 0.5668292045593262,\n",
              " 0.5668291449546814,\n",
              " 0.5668291449546814,\n",
              " 0.5668293237686157,\n",
              " 0.5668291449546814,\n",
              " 0.5668294429779053,\n",
              " 0.5668295621871948,\n",
              " 0.5668294429779053,\n",
              " 0.56682950258255,\n",
              " 0.566829264163971,\n",
              " 0.5668292045593262,\n",
              " 0.5668293833732605,\n",
              " 0.5668293237686157,\n",
              " 0.566829264163971,\n",
              " 0.5668291449546814,\n",
              " 0.5668293237686157,\n",
              " 0.5668294429779053,\n",
              " 0.5668294429779053,\n",
              " 0.566829264163971,\n",
              " 0.5668296217918396,\n",
              " 0.5668296217918396,\n",
              " 0.5668295621871948,\n",
              " 0.56682950258255,\n",
              " 0.5668293237686157,\n",
              " 0.56682950258255,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.56682950258255,\n",
              " 0.5668295621871948,\n",
              " 0.5668293237686157,\n",
              " 0.5668294429779053,\n",
              " 0.5668293833732605,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668297410011292,\n",
              " 0.5668298602104187,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668300986289978,\n",
              " 0.5668299198150635,\n",
              " 0.5668298602104187,\n",
              " 0.5668299794197083,\n",
              " 0.566830039024353,\n",
              " 0.5668298006057739,\n",
              " 0.5668299198150635,\n",
              " 0.5668298006057739,\n",
              " 0.5668298602104187,\n",
              " 0.5668298602104187,\n",
              " 0.5668298602104187,\n",
              " 0.5668299794197083,\n",
              " 0.5668297410011292,\n",
              " 0.566830039024353,\n",
              " 0.5668299794197083,\n",
              " 0.5668299198150635,\n",
              " 0.5668299794197083,\n",
              " 0.5668299794197083,\n",
              " 0.5668298602104187,\n",
              " 0.5668298602104187,\n",
              " 0.5668299198150635,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668299198150635,\n",
              " 0.5668297410011292,\n",
              " 0.5668297410011292,\n",
              " 0.5668297410011292,\n",
              " 0.5668298006057739,\n",
              " 0.5668298602104187,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668297410011292,\n",
              " 0.5668298006057739,\n",
              " 0.5668296813964844,\n",
              " 0.5668296217918396,\n",
              " 0.5668296217918396,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.5668296217918396,\n",
              " 0.5668296217918396,\n",
              " 0.5668298602104187,\n",
              " 0.5668299794197083,\n",
              " 0.5668298602104187,\n",
              " 0.5668298006057739,\n",
              " 0.5668298602104187,\n",
              " 0.5668298006057739,\n",
              " 0.5668299198150635,\n",
              " 0.5668297410011292,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668296217918396,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.56682950258255,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668294429779053,\n",
              " 0.5668294429779053,\n",
              " 0.56682950258255,\n",
              " 0.5668293833732605,\n",
              " 0.5668293237686157,\n",
              " 0.5668291449546814,\n",
              " 0.5668292045593262,\n",
              " 0.5668290257453918,\n",
              " 0.5668290257453918,\n",
              " 0.5668288469314575,\n",
              " 0.5668290257453918,\n",
              " 0.5668288469314575,\n",
              " 0.5668289065361023,\n",
              " 0.5668286085128784,\n",
              " 0.5668284893035889,\n",
              " 0.5668283700942993,\n",
              " 0.5668283700942993,\n",
              " 0.5668285489082336,\n",
              " 0.5668285489082336,\n",
              " 0.5668287873268127,\n",
              " 0.5668287873268127,\n",
              " 0.5668286085128784,\n",
              " 0.566828727722168,\n",
              " 0.5668288469314575,\n",
              " 0.566828727722168,\n",
              " 0.5668288469314575,\n",
              " 0.5668286085128784,\n",
              " 0.5668285489082336,\n",
              " 0.5668285489082336,\n",
              " 0.5668283700942993,\n",
              " 0.5668286085128784,\n",
              " 0.5668286085128784,\n",
              " 0.566828727722168,\n",
              " 0.5668289065361023,\n",
              " 0.5668284893035889,\n",
              " 0.5668288469314575,\n",
              " 0.5668287873268127,\n",
              " 0.5668287873268127,\n",
              " 0.5668289661407471,\n",
              " 0.5668289065361023,\n",
              " 0.5668290257453918,\n",
              " 0.5668289065361023,\n",
              " 0.5668287873268127,\n",
              " 0.5668288469314575,\n",
              " 0.5668292045593262,\n",
              " 0.5668290853500366,\n",
              " 0.5668293833732605,\n",
              " 0.5668290853500366,\n",
              " 0.5668291449546814,\n",
              " 0.5668290257453918,\n",
              " 0.5668290853500366,\n",
              " 0.566829264163971,\n",
              " 0.5668289661407471,\n",
              " 0.5668292045593262,\n",
              " 0.5668289661407471,\n",
              " 0.5668288469314575,\n",
              " 0.5668290853500366,\n",
              " 0.5668292045593262,\n",
              " 0.5668291449546814,\n",
              " 0.5668293833732605,\n",
              " 0.5668292045593262,\n",
              " 0.5668293833732605,\n",
              " 0.5668297410011292,\n",
              " 0.5668298006057739,\n",
              " 0.5668298602104187,\n",
              " 0.5668300986289978,\n",
              " 0.5668299794197083,\n",
              " 0.5668301582336426,\n",
              " 0.5668300986289978,\n",
              " 0.5668302774429321,\n",
              " 0.5668303370475769,\n",
              " 0.5668305158615112,\n",
              " 0.5668305158615112,\n",
              " 0.5668305158615112,\n",
              " 0.5668308734893799,\n",
              " 0.5668309926986694,\n",
              " 0.5668309330940247,\n",
              " 0.5668309926986694,\n",
              " 0.5668308734893799,\n",
              " 0.5668309330940247,\n",
              " 0.5668309926986694,\n",
              " 0.5668307542800903,\n",
              " 0.5668309330940247,\n",
              " 0.5668308138847351,\n",
              " 0.5668309926986694,\n",
              " 0.5668309330940247,\n",
              " 0.5668307542800903,\n",
              " 0.5668307542800903,\n",
              " 0.5668308138847351,\n",
              " 0.5668306350708008,\n",
              " 0.5668306946754456,\n",
              " 0.5668307542800903,\n",
              " 0.5668303966522217,\n",
              " 0.5668305158615112,\n",
              " 0.5668305158615112,\n",
              " 0.5668304562568665,\n",
              " 0.5668304562568665,\n",
              " 0.566830575466156,\n",
              " 0.5668305158615112,\n",
              " 0.566830575466156,\n",
              " 0.566830575466156,\n",
              " 0.5668303966522217,\n",
              " 0.5668303370475769,\n",
              " 0.5668301582336426,\n",
              " 0.5668301582336426,\n",
              " 0.5668303370475769,\n",
              " 0.5668301582336426,\n",
              " 0.5668303370475769,\n",
              " 0.5668303966522217,\n",
              " 0.5668304562568665,\n",
              " 0.5668303966522217,\n",
              " 0.5668305158615112,\n",
              " 0.566830575466156,\n",
              " 0.5668305158615112,\n",
              " 0.566830575466156,\n",
              " 0.5668305158615112,\n",
              " 0.566830575466156,\n",
              " 0.5668305158615112,\n",
              " 0.5668305158615112,\n",
              " 0.5668303370475769,\n",
              " 0.5668302774429321,\n",
              " 0.5668302178382874,\n",
              " 0.5668302178382874,\n",
              " 0.5668301582336426,\n",
              " 0.566830039024353,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.5668299794197083,\n",
              " 0.5668301582336426,\n",
              " 0.566830039024353,\n",
              " 0.566830039024353,\n",
              " 0.5668299794197083,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668296217918396,\n",
              " 0.5668295621871948,\n",
              " 0.566829264163971,\n",
              " 0.5668292045593262,\n",
              " 0.5668294429779053,\n",
              " 0.5668293833732605,\n",
              " 0.5668295621871948,\n",
              " 0.5668294429779053,\n",
              " 0.566829264163971,\n",
              " 0.5668293833732605,\n",
              " 0.5668293833732605,\n",
              " 0.566829264163971,\n",
              " 0.5668293833732605,\n",
              " 0.5668295621871948,\n",
              " 0.5668293237686157,\n",
              " 0.566829264163971,\n",
              " 0.566829264163971,\n",
              " 0.566829264163971,\n",
              " 0.5668293237686157,\n",
              " 0.5668293237686157,\n",
              " 0.5668292045593262,\n",
              " 0.5668292045593262,\n",
              " 0.5668290853500366,\n",
              " 0.5668292045593262,\n",
              " 0.566829264163971,\n",
              " 0.5668293237686157,\n",
              " 0.5668295621871948,\n",
              " 0.5668297410011292,\n",
              " 0.5668296217918396,\n",
              " 0.5668296217918396,\n",
              " 0.5668295621871948,\n",
              " 0.56682950258255,\n",
              " 0.5668294429779053,\n",
              " 0.5668293833732605,\n",
              " 0.5668293237686157,\n",
              " 0.5668293833732605,\n",
              " 0.56682950258255,\n",
              " 0.5668294429779053,\n",
              " 0.5668294429779053,\n",
              " 0.56682950258255,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668297410011292,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668296217918396,\n",
              " 0.5668298006057739,\n",
              " 0.5668295621871948,\n",
              " 0.5668296217918396,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.566830039024353,\n",
              " 0.5668298602104187,\n",
              " 0.566830039024353,\n",
              " 0.5668299794197083,\n",
              " 0.566830039024353,\n",
              " 0.566830039024353,\n",
              " 0.5668299794197083,\n",
              " 0.5668301582336426,\n",
              " 0.5668302178382874,\n",
              " 0.5668299198150635,\n",
              " 0.5668299794197083,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.5668302178382874,\n",
              " 0.566830039024353,\n",
              " 0.5668300986289978,\n",
              " 0.5668301582336426,\n",
              " 0.566830039024353,\n",
              " 0.5668300986289978,\n",
              " 0.5668299198150635,\n",
              " 0.5668298602104187,\n",
              " 0.5668299198150635,\n",
              " 0.5668298602104187,\n",
              " 0.5668297410011292,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668298602104187,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668297410011292,\n",
              " 0.5668298602104187,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668297410011292,\n",
              " 0.5668298006057739,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668296813964844,\n",
              " 0.5668295621871948,\n",
              " 0.56682950258255,\n",
              " 0.5668294429779053,\n",
              " 0.56682950258255,\n",
              " 0.5668295621871948,\n",
              " 0.5668294429779053,\n",
              " 0.5668296217918396,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.5668293237686157,\n",
              " 0.5668294429779053,\n",
              " 0.56682950258255,\n",
              " 0.5668296813964844,\n",
              " 0.5668296217918396,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.5668294429779053,\n",
              " 0.56682950258255,\n",
              " 0.5668293237686157,\n",
              " 0.5668293237686157,\n",
              " 0.5668292045593262,\n",
              " 0.5668292045593262,\n",
              " 0.5668290853500366,\n",
              " 0.5668293237686157,\n",
              " 0.5668293833732605,\n",
              " 0.5668294429779053,\n",
              " 0.5668293833732605,\n",
              " 0.5668292045593262,\n",
              " 0.5668293833732605,\n",
              " 0.5668295621871948,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668298006057739,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668297410011292,\n",
              " 0.5668299794197083,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.5668294429779053,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668296217918396,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.56682950258255,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.56682950258255,\n",
              " 0.5668295621871948,\n",
              " 0.5668293237686157,\n",
              " 0.5668291449546814,\n",
              " 0.5668293237686157,\n",
              " 0.56682950258255,\n",
              " 0.5668293237686157,\n",
              " 0.5668293833732605,\n",
              " 0.56682950258255,\n",
              " 0.56682950258255,\n",
              " 0.5668294429779053,\n",
              " 0.5668293833732605,\n",
              " 0.5668293833732605,\n",
              " 0.5668293237686157,\n",
              " 0.56682950258255,\n",
              " 0.5668294429779053,\n",
              " 0.5668293237686157,\n",
              " 0.5668293237686157,\n",
              " 0.5668295621871948,\n",
              " 0.5668293833732605,\n",
              " 0.56682950258255,\n",
              " 0.5668295621871948,\n",
              " 0.5668295621871948,\n",
              " 0.5668296217918396,\n",
              " 0.5668294429779053,\n",
              " 0.5668296813964844,\n",
              " 0.5668298602104187,\n",
              " 0.5668298602104187,\n",
              " 0.566830039024353,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.566830039024353,\n",
              " 0.566830039024353,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668298602104187,\n",
              " 0.5668298602104187,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.5668297410011292,\n",
              " 0.5668299198150635,\n",
              " 0.566830039024353,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.5668301582336426,\n",
              " 0.566830039024353,\n",
              " 0.5668298602104187,\n",
              " 0.5668298602104187,\n",
              " 0.5668299198150635,\n",
              " 0.5668299198150635,\n",
              " 0.5668302178382874,\n",
              " 0.566830039024353,\n",
              " 0.5668300986289978,\n",
              " 0.5668299198150635,\n",
              " 0.5668297410011292,\n",
              " 0.5668297410011292,\n",
              " 0.5668295621871948,\n",
              " 0.56682950258255,\n",
              " 0.5668294429779053,\n",
              " 0.5668292045593262,\n",
              " 0.5668293237686157,\n",
              " 0.5668293833732605,\n",
              " 0.5668293833732605,\n",
              " 0.5668296217918396,\n",
              " 0.566829264163971,\n",
              " 0.566829264163971,\n",
              " 0.5668287873268127,\n",
              " 0.5668289661407471,\n",
              " 0.5668288469314575,\n",
              " 0.5668288469314575,\n",
              " 0.566828727722168,\n",
              " 0.5668286085128784,\n",
              " 0.5668283700942993,\n",
              " 0.566828727722168,\n",
              " 0.5668286085128784,\n",
              " 0.5668286085128784,\n",
              " 0.5668286085128784,\n",
              " 0.5668284893035889,\n",
              " 0.566828191280365,\n",
              " 0.5668284893035889,\n",
              " 0.566828727722168,\n",
              " 0.5668284893035889,\n",
              " 0.5668286085128784,\n",
              " 0.5668285489082336,\n",
              " 0.5668287873268127,\n",
              " 0.566828727722168,\n",
              " 0.5668289065361023,\n",
              " 0.5668292045593262,\n",
              " 0.5668290853500366,\n",
              " 0.5668287873268127,\n",
              " 0.5668290853500366,\n",
              " 0.5668286681175232,\n",
              " 0.5668289661407471,\n",
              " 0.5668289065361023,\n",
              " 0.5668287873268127,\n",
              " 0.5668290257453918,\n",
              " 0.5668288469314575,\n",
              " 0.5668289065361023,\n",
              " 0.5668293237686157,\n",
              " 0.566829264163971,\n",
              " 0.5668298006057739,\n",
              " 0.5668294429779053,\n",
              " 0.5668291449546814,\n",
              " 0.5668293833732605,\n",
              " 0.5668289661407471,\n",
              " 0.566829264163971,\n",
              " 0.566829264163971,\n",
              " 0.566829264163971,\n",
              " 0.5668292045593262,\n",
              " 0.5668292045593262,\n",
              " 0.5668294429779053,\n",
              " 0.5668293833732605,\n",
              " 0.5668293237686157,\n",
              " 0.5668294429779053,\n",
              " 0.56682950258255,\n",
              " 0.56682950258255,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.566830039024353,\n",
              " 0.5668301582336426,\n",
              " 0.5668302178382874,\n",
              " 0.5668303966522217,\n",
              " 0.5668306350708008,\n",
              " 0.5668303966522217,\n",
              " 0.5668305158615112,\n",
              " 0.5668306946754456,\n",
              " 0.5668310523033142,\n",
              " 0.566831111907959,\n",
              " 0.5668308734893799,\n",
              " 0.5668308734893799,\n",
              " 0.5668308734893799,\n",
              " 0.5668311715126038,\n",
              " 0.5668312311172485,\n",
              " 0.5668309926986694,\n",
              " 0.5668312311172485,\n",
              " 0.5668307542800903,\n",
              " 0.566830575466156,\n",
              " 0.5668305158615112,\n",
              " 0.5668305158615112,\n",
              " 0.5668303966522217,\n",
              " 0.5668304562568665,\n",
              " 0.5668302774429321,\n",
              " 0.5668302178382874,\n",
              " 0.5668299198150635,\n",
              " 0.566830039024353,\n",
              " 0.5668298602104187,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668296217918396,\n",
              " 0.5668298006057739,\n",
              " 0.5668295621871948,\n",
              " 0.56682950258255,\n",
              " 0.5668294429779053,\n",
              " 0.5668297410011292,\n",
              " 0.5668293237686157,\n",
              " 0.56682950258255,\n",
              " 0.566829264163971,\n",
              " 0.5668292045593262,\n",
              " 0.5668293833732605,\n",
              " 0.5668293833732605,\n",
              " 0.56682950258255,\n",
              " 0.5668296217918396,\n",
              " 0.56682950258255,\n",
              " 0.5668295621871948,\n",
              " 0.56682950258255,\n",
              " 0.5668295621871948,\n",
              " 0.5668298006057739,\n",
              " 0.5668298006057739,\n",
              " 0.5668298602104187,\n",
              " 0.5668299198150635,\n",
              " 0.5668298006057739,\n",
              " 0.5668299794197083,\n",
              " 0.5668298602104187,\n",
              " 0.5668298602104187,\n",
              " 0.5668298602104187,\n",
              " 0.5668296813964844,\n",
              " 0.5668298602104187,\n",
              " 0.5668297410011292,\n",
              " 0.5668299198150635,\n",
              " 0.5668297410011292,\n",
              " 0.5668296217918396,\n",
              " 0.5668296217918396,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668294429779053,\n",
              " 0.5668295621871948,\n",
              " 0.56682950258255,\n",
              " 0.5668297410011292,\n",
              " 0.5668296217918396,\n",
              " 0.5668298006057739,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.5668298006057739,\n",
              " 0.5668299198150635,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.5668299794197083,\n",
              " 0.5668299198150635,\n",
              " 0.5668298006057739,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668296813964844,\n",
              " 0.5668295621871948,\n",
              " 0.5668296813964844,\n",
              " 0.566830039024353,\n",
              " 0.56682950258255,\n",
              " 0.5668297410011292,\n",
              " 0.5668296813964844,\n",
              " 0.5668298602104187,\n",
              " 0.5668299198150635,\n",
              " 0.5668298602104187,\n",
              " 0.5668296217918396,\n",
              " 0.5668296813964844,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "pd.Series(my_pred.flatten()).plot()\n",
        "plt.axhline(y=0.5,color='red')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "6cp--umJCEXi",
        "outputId": "7777105b-6e12-4ce5-dfe1-3a9b0eb24116"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7fd880c06390>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = my_pred.copy()\n",
        "pred[pred>=0.5]=1\n",
        "pred[pred<0.5]=0\n",
        "(pred.flatten()==my_y_test).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBdvDj_xCXVy",
        "outputId": "0bb79047-a9ee-4de4-9827-ed3f212cf7f9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5225806451612903"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def decode_from_norm(std_change,min_max):\n",
        "#   min, max = min_max\n",
        "#   return std_change*(max-min)+min"
      ],
      "metadata": {
        "id": "eMOf5eXaepMA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(15,10))\n",
        "# my_pred_dec = decode_from_norm(my_pred,my_min_max)\n",
        "# my_y_test_dec = decode_from_norm(my_y_test,my_min_max)\n",
        "# pd.Series(my_y_test_dec).plot(label=\"True percentage change!\")\n",
        "# pd.Series(my_pred_dec.flatten()).plot(label=\"Predicted percentage change!\")\n",
        "# plt.axhline(y=0,color='red')\n",
        "# plt.title(\"Stock Adj.Close predicted change!\")\n",
        "# plt.legend()"
      ],
      "metadata": {
        "id": "sUJyxJKVfZ9w"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}